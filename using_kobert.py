# ================================================================
# ğŸš€ ì§„í™”ëœ ê³ ì„±ëŠ¥ KoBERT (0.83+ ì½”ë“œ ê°œì„  ë²„ì „)
# ================================================================

!pip install transformers==4.36.0
!pip install torch torchvision torchaudio
!pip install scikit-learn
!pip install pandas numpy tqdm

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
from tqdm import tqdm
import warnings
import random
import os
import gc
from datetime import datetime
import math

warnings.filterwarnings('ignore')

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ğŸ”¥ Using device: {device}')

# íŒŒì¼ ê²½ë¡œ
DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks'
DATA_PATH = f'{DRIVE_PATH}'
RESULT_PATH = f'{DRIVE_PATH}/results'
os.makedirs(RESULT_PATH, exist_ok=True)

# ğŸ¯ ì§„í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° (í˜„ì¬ í™˜ê²½ ìµœì í™”)
MAX_LEN = 512
BATCH_SIZE = 10          # Colab í™˜ê²½ ê³ ë ¤ (ì›ë˜ 12ì—ì„œ ì¡°ì •)
GRADIENT_ACCUMULATION_STEPS = 2  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° 20
EPOCHS = 6               # ë” ì¶©ë¶„í•œ í•™ìŠµ
LEARNING_RATE = 1.5e-5   # ì•½ê°„ ë‚®ì¶˜ í•™ìŠµë¥ ë¡œ ì•ˆì •ì„±
WARMUP_RATIO = 0.15      # ë” ê¸´ ì›Œë°ì—…
WEIGHT_DECAY = 0.01

MODEL_NAME = 'skt/kobert-base-v1'

# ğŸ¯ ê°œì„ ëœ ì²­í‚¹ ì„¤ì •
CHUNK_SIZE = 420         # ì•½ê°„ ì¦ê°€
OVERLAP_SIZE = 80        # ìµœì í™”ëœ ì˜¤ë²„ë©
MAX_CHUNKS = 4           # ê²€ì¦ëœ ì„¤ì • ìœ ì§€

# ================================================================
# ì§„í™”ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹
# ================================================================

class EvolvedLongTextDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, max_len=MAX_LEN, augment=False):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augment = augment  # ìƒˆë¡œìš´ ì¦ê°• ê¸°ëŠ¥
        
    def __len__(self):
        return len(self.texts)
    
    def _enhanced_chunk_text(self, text):
        """ê°œì„ ëœ í…ìŠ¤íŠ¸ ì²­í‚¹ (ì›ë˜ ë°©ì‹ + ì•ˆì „ì„± ê°•í™”)"""
        if not text.strip():
            return [""]
        
        # ì›ë˜ ë°©ì‹: ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
        sentences = text.split('. ')
        if len(sentences) == 1:
            sentences = text.split('.')
        
        # í† í° ê¸°ë°˜ ì²­í‚¹ (ê²€ì¦ëœ ë°©ì‹)
        try:
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
        except:
            # í† í¬ë‚˜ì´ì € ì—ëŸ¬ ì‹œ ì•ˆì „í•œ ì²˜ë¦¬
            return [text[:self.max_len-50]]
        
        if len(tokens) <= self.max_len - 2:
            return [text]
        
        chunks = []
        current_chunk_tokens = []
        current_length = 0
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            try:
                sentence_tokens = self.tokenizer.encode(sentence + '.', add_special_tokens=False)
            except:
                continue
            
            if current_length + len(sentence_tokens) > CHUNK_SIZE:
                if current_chunk_tokens:
                    chunk_text = self.tokenizer.decode(current_chunk_tokens, skip_special_tokens=True)
                    chunks.append(chunk_text)
                    # ê°œì„ ëœ ì˜¤ë²„ë© (ì •ë³´ ì†ì‹¤ ìµœì†Œí™”)
                    overlap_tokens = current_chunk_tokens[-OVERLAP_SIZE:] if len(current_chunk_tokens) > OVERLAP_SIZE else []
                    current_chunk_tokens = overlap_tokens + sentence_tokens
                    current_length = len(current_chunk_tokens)
                else:
                    current_chunk_tokens = sentence_tokens
                    current_length = len(sentence_tokens)
            else:
                current_chunk_tokens.extend(sentence_tokens)
                current_length += len(sentence_tokens)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk_tokens:
            chunk_text = self.tokenizer.decode(current_chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
        
        return chunks if chunks else [text[:CHUNK_SIZE*2]]
    
    def _simple_augment(self, text):
        """ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°ì´í„° ì¦ê°•"""
        if not self.augment or random.random() > 0.2:
            return text
        
        # 1. ê³µë°± íŒ¨í„´ ë³€ê²½ (AI í…ìŠ¤íŠ¸ íƒì§€ì— ì¤‘ìš”)
        if random.random() > 0.5:
            text = text.replace('  ', ' ')  # ì´ì¤‘ ê³µë°± ì œê±°
            
        # 2. êµ¬ë‘ì  ì •ê·œí™”
        if random.random() > 0.7:
            text = text.replace('...', '.')
            text = text.replace('!!', '!')
            
        return text
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        
        # ë°ì´í„° ì¦ê°• ì ìš©
        text = self._simple_augment(text)
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹ (ê²€ì¦ëœ ë°©ì‹)
        chunks = self._enhanced_chunk_text(text)
        
        # ê° ì²­í¬ í† í¬ë‚˜ì´ì§•
        chunk_encodings = []
        for chunk in chunks:
            try:
                encoding = self.tokenizer(
                    chunk,
                    add_special_tokens=True,
                    max_length=self.max_len,
                    padding='max_length',
                    truncation=True,
                    return_attention_mask=True,
                    return_tensors='pt'
                )
                chunk_encodings.append({
                    'input_ids': encoding['input_ids'].flatten(),
                    'attention_mask': encoding['attention_mask'].flatten()
                })
            except:
                # ì—ëŸ¬ ì‹œ ë¹ˆ ì²­í¬ ì¶”ê°€
                chunk_encodings.append({
                    'input_ids': torch.zeros(self.max_len, dtype=torch.long),
                    'attention_mask': torch.zeros(self.max_len, dtype=torch.long)
                })
        
        # íŒ¨ë”©
        while len(chunk_encodings) < MAX_CHUNKS:
            chunk_encodings.append({
                'input_ids': torch.zeros(self.max_len, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_len, dtype=torch.long)
            })
        
        chunk_encodings = chunk_encodings[:MAX_CHUNKS]
        
        result = {
            'input_ids': torch.stack([chunk['input_ids'] for chunk in chunk_encodings]),
            'attention_mask': torch.stack([chunk['attention_mask'] for chunk in chunk_encodings]),
            'num_chunks': torch.tensor(min(len(chunks), MAX_CHUNKS), dtype=torch.long)
        }
        
        if self.labels is not None:
            result['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
            
        return result

# ================================================================
# ì§„í™”ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜
# ================================================================

class EvolvedKoBERTClassifier(nn.Module):
    def __init__(self, model_name, num_classes=2, dropout_rate=0.15):
        super(EvolvedKoBERTClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        
        # ê²€ì¦ëœ ë ˆì´ì–´ ê³ ì • (6ê°œ ìœ ì§€)
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:6]:
            for param in layer.parameters():
                param.requires_grad = False
        
        hidden_size = self.bert.config.hidden_size
        
        # ê°œì„ ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.chunk_attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, batch_first=True, dropout=dropout_rate
        )
        
        # ìœ„ì¹˜ ì¸ì½”ë”© (ê²€ì¦ëœ ìš”ì†Œ)
        self.position_embeddings = nn.Embedding(MAX_CHUNKS, hidden_size)
        
        # ğŸ†• ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ë„¤íŠ¸ì›Œí¬ (ìƒˆ ê¸°ëŠ¥)
        self.chunk_importance = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )
        
        # ğŸ†• ì ì‘ì  í’€ë§ (ìƒˆ ê¸°ëŠ¥)
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # ê¸°ì¡´ ê²€ì¦ëœ ë¶„ë¥˜ê¸° êµ¬ì¡° ìœ ì§€ + ê°œì„ 
        self.pre_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU(),  # ReLU â†’ GELU (ì„±ëŠ¥ í–¥ìƒ)
            nn.Dropout(dropout_rate)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.GELU(),
            nn.Dropout(dropout_rate // 2),
            nn.Linear(hidden_size // 4, num_classes)
        )
        
        # ğŸ†• ì¶œë ¥ ë³´ì • (ìƒˆ ê¸°ëŠ¥)
        self.output_calibration = nn.Sequential(
            nn.Linear(num_classes, num_classes),
            nn.Dropout(dropout_rate // 4)
        )
        
    def forward(self, input_ids, attention_mask, num_chunks):
        batch_size, num_chunks_max, seq_len = input_ids.shape
        
        with torch.cuda.amp.autocast():
            # ê¸°ì¡´ ê²€ì¦ëœ ì¸ì½”ë”© ë°©ì‹
            input_ids_flat = input_ids.view(-1, seq_len)
            attention_mask_flat = attention_mask.view(-1, seq_len)
            
            outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
            
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                chunk_embeddings = outputs.pooler_output
            else:
                chunk_embeddings = outputs.last_hidden_state[:, 0, :]
            
            chunk_embeddings = chunk_embeddings.view(batch_size, num_chunks_max, -1)
            
            # ê²€ì¦ëœ ìœ„ì¹˜ ì¸ì½”ë”©
            positions = torch.arange(num_chunks_max, device=chunk_embeddings.device).unsqueeze(0).repeat(batch_size, 1)
            position_embeddings = self.position_embeddings(positions)
            chunk_embeddings = chunk_embeddings + position_embeddings
            
            # ì²­í¬ ë§ˆìŠ¤í¬ (ê²€ì¦ëœ ë°©ì‹)
            chunk_mask = torch.zeros(batch_size, num_chunks_max, device=chunk_embeddings.device)
            for i, num_chunk in enumerate(num_chunks):
                chunk_mask[i, :num_chunk] = 1
            
            # ğŸ†• ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° (ìƒˆ ê¸°ëŠ¥)
            chunk_importance = self.chunk_importance(chunk_embeddings).squeeze(-1)
            chunk_importance = chunk_importance * chunk_mask  # ë§ˆìŠ¤í‚¹ ì ìš©
            
            # ê¸°ì¡´ ì–´í…ì…˜ + ì¤‘ìš”ë„ ê²°í•©
            attended_chunks, attention_weights = self.chunk_attention(
                chunk_embeddings, chunk_embeddings, chunk_embeddings,
                key_padding_mask=(chunk_mask == 0)
            )
            
            # ğŸ†• ì ì‘ì  ê°€ì¤‘ í‰ê·  (ê°œì„ ëœ í’€ë§)
            doc_mask = chunk_mask.unsqueeze(-1)
            
            # ì–´í…ì…˜ê³¼ ì¤‘ìš”ë„ ê²°í•©
            combined_weights = (attention_weights.mean(dim=1) + chunk_importance) / 2
            combined_weights = F.softmax(combined_weights.masked_fill(chunk_mask == 0, float('-inf')), dim=1)
            
            weighted_chunks = attended_chunks * combined_weights.unsqueeze(-1)
            doc_embedding = weighted_chunks.sum(dim=1)
            
            # ê¸°ì¡´ ê²€ì¦ëœ ë¶„ë¥˜ ê³¼ì •
            doc_embedding = doc_embedding + self.pre_classifier(doc_embedding)
            logits = self.classifier(doc_embedding)
            
            # ğŸ†• ì¶œë ¥ ë³´ì • (ì„±ëŠ¥ ë¯¸ì„¸ ì¡°ì •)
            calibrated_logits = logits + self.output_calibration(logits)
        
        return calibrated_logits, attention_weights

# ================================================================
# ì§„í™”ëœ í›ˆë ¨ í•¨ìˆ˜ë“¤
# ================================================================

def evolved_train_epoch(model, data_loader, optimizer, criterion, scheduler, device, scaler, epoch):
    """ì§„í™”ëœ í›ˆë ¨ ì—í¬í¬ (gradient accumulation ì¶”ê°€)"""
    model.train()
    total_loss = 0
    correct_predictions = 0
    accumulated_loss = 0
    
    optimizer.zero_grad()
    
    for batch_idx, batch in enumerate(tqdm(data_loader, desc=f'Training Epoch {epoch}')):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        num_chunks = batch['num_chunks'].to(device)
        
        with torch.cuda.amp.autocast():
            logits, attention_weights = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                num_chunks=num_chunks
            )
            loss = criterion(logits, labels) / GRADIENT_ACCUMULATION_STEPS
        
        _, preds = torch.max(logits, dim=1)
        correct_predictions += torch.sum(preds == labels)
        accumulated_loss += loss.item()
        
        # Gradient accumulation
        scaler.scale(loss).backward()
        
        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad()
            
            total_loss += accumulated_loss
            accumulated_loss = 0
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ìì£¼)
        if batch_idx % 15 == 0:
            torch.cuda.empty_cache()
    
    # ë‚¨ì€ gradient ì²˜ë¦¬
    if accumulated_loss > 0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
        total_loss += accumulated_loss
    
    avg_loss = total_loss / (len(data_loader) // GRADIENT_ACCUMULATION_STEPS)
    accuracy = correct_predictions.double() / len(data_loader.dataset)
    
    return avg_loss, accuracy

def evolved_eval_model(model, data_loader, criterion, device):
    """ì§„í™”ëœ í‰ê°€ í•¨ìˆ˜ (ë” ìƒì„¸í•œ ë©”íŠ¸ë¦­)"""
    model.eval()
    total_loss = 0
    correct_predictions = 0
    predictions = []
    probabilities = []
    real_values = []
    confidence_scores = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader, desc='Evaluating')):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            
            with torch.cuda.amp.autocast():
                logits, attention_weights = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    num_chunks=num_chunks
                )
                loss = criterion(logits, labels)
            
            _, preds = torch.max(logits, dim=1)
            correct_predictions += torch.sum(preds == labels)
            total_loss += loss.item()
            
            probs = torch.softmax(logits, dim=1)
            
            # ğŸ†• ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            max_probs = torch.max(probs, dim=1)[0]
            confidence_scores.extend(max_probs.cpu().numpy())
            
            predictions.extend(preds.cpu().numpy())
            probabilities.extend(probs[:, 1].cpu().numpy())
            real_values.extend(labels.cpu().numpy())
            
            if batch_idx % 15 == 0:
                torch.cuda.empty_cache()
    
    f1 = f1_score(real_values, predictions)
    avg_confidence = np.mean(confidence_scores)
    
    return (total_loss / len(data_loader), 
            correct_predictions.double() / len(data_loader.dataset),
            predictions, probabilities, real_values, f1, avg_confidence)

def main():
    print("ğŸš€ ì§„í™”ëœ ê³ ì„±ëŠ¥ KoBERT (0.83+ ê¸°ë°˜ ê°œì„ )")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv(f'{DATA_PATH}/train.csv', encoding='utf-8-sig')
    test = pd.read_csv(f'{DATA_PATH}/test.csv', encoding='utf-8-sig')
    
    print(f"ğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"   Train: {train.shape}")
    print(f"   Test: {test.shape}")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê²€ì¦ëœ ë°©ì‹ ìœ ì§€)
    train['title'] = train['title'].fillna('').str.strip()
    train['full_text'] = train['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    train['combined_text'] = train['title'] + ' [SEP] ' + train['full_text']
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    text_lengths = train['combined_text'].str.len()
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´ - í‰ê· : {text_lengths.mean():.0f}, ìµœëŒ€: {text_lengths.max():,}")
    print(f"   ê¸´ í…ìŠ¤íŠ¸ (>2000ì): {(text_lengths > 2000).sum()}/{len(train)}")
    
    X = train['combined_text']
    y = train['generated']
    
    print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(y.value_counts())}")
    
    # ë°ì´í„° ë¶„í•  (ê²€ì¦ëœ ë¹„ìœ¨)
    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
    
    # í† í¬ë‚˜ì´ì €
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        print(f"âœ… {MODEL_NAME} í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ KoBERT ë¡œë“œ ì‹¤íŒ¨: {e}")
        MODEL_NAME = 'klue/bert-base'
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        print(f"   ëŒ€ì²´: {MODEL_NAME}")
    
    # ë°ì´í„°ì…‹ (í›ˆë ¨ìš©ì€ ì¦ê°• ì ìš©)
    train_dataset = EvolvedLongTextDataset(X_train, y_train, tokenizer, MAX_LEN, augment=True)
    val_dataset = EvolvedLongTextDataset(X_val, y_val, tokenizer, MAX_LEN, augment=False)
    
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=2, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=2, pin_memory=True
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = EvolvedKoBERTClassifier(MODEL_NAME, num_classes=2)
    model = model.to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ—ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,}")
    
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    criterion = nn.CrossEntropyLoss()
    scaler = torch.cuda.amp.GradScaler()
    
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=LEARNING_RATE, 
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-6
    )
    
    # ğŸ†• ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ (ë” ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œ)
    effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS
    total_steps = (len(train_loader) // GRADIENT_ACCUMULATION_STEPS) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"\nğŸ¯ ì§„í™”ëœ ì„¤ì •:")
    print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {effective_batch_size}")
    print(f"   ì—í¬í¬: {EPOCHS}")
    print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
    print(f"   ìŠ¤ì¼€ì¤„ëŸ¬: Cosine with warmup")
    print(f"   ìƒˆ ê¸°ëŠ¥: ì²­í¬ ì¤‘ìš”ë„, ì ì‘ì  í’€ë§, ì¶œë ¥ ë³´ì •")
    
    # í›ˆë ¨ ì‹¤í–‰
    best_score = 0
    best_model = None
    
    print("\nğŸš€ ì§„í™”ëœ í›ˆë ¨ ì‹œì‘!")
    
    for epoch in range(EPOCHS):
        print(f'\n=== Epoch {epoch + 1}/{EPOCHS} ===')
        
        # í›ˆë ¨
        train_loss, train_acc = evolved_train_epoch(
            model, train_loader, optimizer, criterion, scheduler, device, scaler, epoch + 1
        )
        
        # ê²€ì¦
        val_loss, val_acc, val_preds, val_probs, val_labels, val_f1, avg_confidence = evolved_eval_model(
            model, val_loader, criterion, device
        )
        
        val_auc = roc_auc_score(val_labels, val_probs)
        
        print(f'ğŸ“ˆ Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}')
        print(f'ğŸ“ˆ Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')
        print(f'ğŸ¯ Metrics - AUC: {val_auc:.4f}, F1: {val_f1:.4f}')
        print(f'ğŸ’¡ ì‹ ë¢°ë„: {avg_confidence:.4f}')
        
        # ğŸ†• ê°œì„ ëœ ë³µí•© ì ìˆ˜ (ì‹ ë¢°ë„ ì¶”ê°€)
        combined_score = 0.6 * val_auc + 0.3 * val_f1 + 0.1 * avg_confidence
        
        if combined_score > best_score:
            best_score = combined_score
            best_model = model.state_dict().copy()
            print(f'â­ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ë³µí•©ì ìˆ˜: {combined_score:.4f}')
        
        torch.cuda.empty_cache()
    
    print(f'\nğŸ† ìµœê³  ë³µí•© ì ìˆ˜: {best_score:.4f}')
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    model.load_state_dict(best_model)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('').str.strip()
    test['full_text'] = test['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    test['combined_text'] = test['title'] + ' [SEP] ' + test['full_text']
    
    test_dataset = EvolvedLongTextDataset(test['combined_text'], labels=None, tokenizer=tokenizer, max_len=MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
    
    # ì˜ˆì¸¡
    test_probabilities = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc='ğŸ”® ì§„í™”ëœ ì˜ˆì¸¡'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            
            with torch.cuda.amp.autocast():
                logits, _ = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    num_chunks=num_chunks
                )
            
            probs = torch.softmax(logits, dim=1)
            test_probabilities.extend(probs[:, 1].cpu().numpy())
    
    # ì˜ˆì¸¡ í†µê³„
    print(f"\nğŸ“Š ì˜ˆì¸¡ í†µê³„:")
    print(f"   í‰ê· : {np.mean(test_probabilities):.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {np.std(test_probabilities):.4f}")
    print(f"   ë²”ìœ„: {min(test_probabilities):.4f} ~ {max(test_probabilities):.4f}")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    sample_submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = test_probabilities
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_path = f'{RESULT_PATH}/evolved_submission_{timestamp}.csv'
    sample_submission.to_csv(submission_path, index=False)
    
    baseline_path = f'{RESULT_PATH}/baseline_submission.csv'
    sample_submission.to_csv(baseline_path, index=False)
    
    print(f"\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥:")
    print(f"   ğŸ“ {submission_path}")
    print(f"   ğŸ“ {baseline_path}")
    
    print(f"\nğŸ‰ ì§„í™”ëœ KoBERT ì™„ë£Œ!")
    print(f"ğŸ¯ ëª©í‘œ: 0.83+ â†’ 0.85+ ë‹¬ì„±")
    print(f"ğŸ†• ì¶”ê°€ ê¸°ëŠ¥: ì²­í¬ ì¤‘ìš”ë„, ì ì‘ì  í’€ë§, ì¶œë ¥ ë³´ì •, ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬")
    
    torch.cuda.empty_cache()
    return model, best_score, test_probabilities

if __name__ == "__main__":
    model, score, preds = main()
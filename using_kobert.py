# ================================================================
# ğŸš€ ì •êµí•œ KoBERT Fine-tuning (í•œêµ­ì–´ íŠ¹í™” ìµœì í™”)
# ================================================================

!pip install transformers==4.36.0
!pip install torch torchvision torchaudio
!pip install scikit-learn
!pip install pandas numpy tqdm

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from tqdm import tqdm
import warnings
import random
import os
import re
import gc
from datetime import datetime
import math

warnings.filterwarnings('ignore')

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ğŸ”¥ Using device: {device}')
if torch.cuda.is_available():
    print(f'   GPU: {torch.cuda.get_device_name(0)}')
    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks'
DATA_PATH = f'{DRIVE_PATH}'
MODEL_PATH = f'{DRIVE_PATH}/saved_models'
RESULT_PATH = f'{DRIVE_PATH}/results'

os.makedirs(MODEL_PATH, exist_ok=True)
os.makedirs(RESULT_PATH, exist_ok=True)

# ğŸ¯ KoBERT ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°
MAX_LEN = 512
BATCH_SIZE = 12  # KoBERTì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
EPOCHS = 5  # ì¶©ë¶„í•œ í•™ìŠµ
LEARNING_RATE = 3e-5  # KoBERTì— ì í•©í•œ í•™ìŠµë¥ 
WARMUP_RATIO = 0.15  # ë” ê¸´ ì›Œë°ì—…
WEIGHT_DECAY = 0.05  # ì ì ˆí•œ regularization
DROPOUT_RATE = 0.3  # ê· í˜•ì¡íŒ ë“œë¡­ì•„ì›ƒ

# ì²­í‚¹ ì„¤ì • (KoBERT íŠ¹ì„± ê³ ë ¤)
CHUNK_SIZE = 400
OVERLAP_SIZE = 50
MAX_CHUNKS = 5

# KoBERT ëª¨ë¸
MODEL_NAME = 'skt/kobert-base-v1'

# ================================================================
# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°
# ================================================================

class KoreanTextPreprocessor:
    """í•œêµ­ì–´ íŠ¹í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    
    @staticmethod
    def clean_korean_text(text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if pd.isna(text) or text == '':
            return ''
        
        # í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤í•œ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°±
        text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?;:()\-\"\'\/]', ' ', text)  # í•œêµ­ì–´ ë³´ì¡´
        text = re.sub(r'\.{3,}', '...', text)  # ì—°ì† ì 
        text = re.sub(r'!{2,}', '!', text)  # ì—°ì† ëŠë‚Œí‘œ
        text = re.sub(r'\?{2,}', '?', text)  # ì—°ì† ë¬¼ìŒí‘œ
        
        # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡° ê³ ë ¤
        text = re.sub(r'([ê°€-í£])\s+([ê°€-í£])', r'\1\2', text)  # ë¶ˆí•„ìš”í•œ í•œê¸€ ê°„ ê³µë°±
        
        return text.strip()
    
    @staticmethod
    def extract_korean_features(text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ"""
        if not text:
            return {}
        
        # í•œêµ­ì–´ íŠ¹ì„± ë¶„ì„
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(text)
        
        # ë¬¸ì¥ ì¢…ê²°ì–´ë¯¸ íŒ¨í„´ (AIê°€ ìì£¼ ì‹¤ìˆ˜í•˜ëŠ” ë¶€ë¶„)
        ending_patterns = {
            'formal_endings': len(re.findall(r'[ê°€-í£]+(ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|ì˜€ìŠµë‹ˆë‹¤|ìˆìŠµë‹ˆë‹¤)\.', text)),
            'informal_endings': len(re.findall(r'[ê°€-í£]+(ì´ì•¼|ì•¼|ë‹¤|ì–´|ì•„)\.', text)),
            'question_endings': len(re.findall(r'[ê°€-í£]+(ê¹Œ|ë‹ˆ|ë‚˜)\?', text)),
        }
        
        # ì¡°ì‚¬ ì‚¬ìš© íŒ¨í„´
        particles = len(re.findall(r'[ê°€-í£]+(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì˜|ë¡œ|ì™€|ê³¼|ë„)', text))
        
        # ì–´ìˆœ ë° ë¬¸ì²´ íŠ¹ì§•
        features = {
            'korean_ratio': korean_chars / total_chars if total_chars > 0 else 0,
            'avg_sentence_length': len(text.split('.')) if '.' in text else 1,
            'particle_density': particles / korean_chars if korean_chars > 0 else 0,
            'formal_ratio': ending_patterns['formal_endings'] / len(text.split('.')) if '.' in text else 0,
            'punctuation_variety': len(set(re.findall(r'[.,!?;:]', text))) / 6,  # êµ¬ë‘ì  ë‹¤ì–‘ì„±
        }
        
        return features

class OptimizedKoreanDataset(Dataset):
    """í•œêµ­ì–´ ìµœì í™” ë°ì´í„°ì…‹"""
    
    def __init__(self, texts, labels=None, tokenizer=None, max_len=MAX_LEN, augment=False):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augment = augment
        self.preprocessor = KoreanTextPreprocessor()
        
    def __len__(self):
        return len(self.texts)
    
    def _smart_korean_chunk(self, text):
        """í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í‚¹"""
        if not text or len(text.strip()) == 0:
            return [""]
        
        # í† í° ê¸¸ì´ í™•ì¸
        tokens = self.tokenizer.encode(text, add_special_tokens=True, truncation=False)
        
        if len(tokens) <= self.max_len:
            return [text]
        
        # í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ ì¸ì‹ (ë” ì •êµí•˜ê²Œ)
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ + ê³µë°±/ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• 
        sentences = re.split(r'([.!?])\s+', text)
        
        # ë¶„í• ëœ ê²°ê³¼ë¥¼ ì¬ê²°í•© (êµ¬ë‘ì  í¬í•¨)
        proper_sentences = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                sentence = sentences[i] + sentences[i+1]
                if sentence.strip():
                    proper_sentences.append(sentence.strip())
        
        if not proper_sentences:
            proper_sentences = [text]
        
        chunks = []
        current_chunk = ""
        
        for sentence in proper_sentences:
            test_chunk = current_chunk + (" " + sentence if current_chunk else sentence)
            test_tokens = self.tokenizer.encode(test_chunk, add_special_tokens=True, truncation=False)
            
            if len(test_tokens) > CHUNK_SIZE and current_chunk:
                chunks.append(current_chunk.strip())
                # ì˜¤ë²„ë© ì¶”ê°€ (í•œêµ­ì–´ ë¬¸ë§¥ ë³´ì¡´)
                words = current_chunk.split()
                overlap_words = words[-OVERLAP_SIZE//10:] if len(words) > OVERLAP_SIZE//10 else []
                current_chunk = " ".join(overlap_words) + " " + sentence if overlap_words else sentence
            else:
                current_chunk = test_chunk
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        if not chunks:
            # ê°•ì œ ë¶„í• 
            truncated_tokens = tokens[:CHUNK_SIZE-2]
            chunks = [self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)]
        
        return chunks[:MAX_CHUNKS]
    
    def _simple_augment(self, text):
        """ê°„ë‹¨í•œ ë°ì´í„° ì¦ê°• (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)"""
        if not self.augment or random.random() > 0.3:
            return text
        
        # ì¡°ì‚¬ ë³€ê²½ (ì€/ëŠ”, ì´/ê°€ ë“±)
        augmented = text
        
        # ê°„ë‹¨í•œ ë™ì˜ì–´ ëŒ€ì²´ (ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ)
        replacements = {
            'ê·¸ë¦¬ê³ ': 'ë˜í•œ',
            'í•˜ì§€ë§Œ': 'ê·¸ëŸ¬ë‚˜',
            'ë•Œë¬¸ì—': 'ì´ìœ ë¡œ',
            'ë”°ë¼ì„œ': 'ê·¸ëŸ¬ë¯€ë¡œ'
        }
        
        for original, replacement in replacements.items():
            if original in augmented and random.random() > 0.7:
                augmented = augmented.replace(original, replacement, 1)
        
        return augmented
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx] if hasattr(self.texts, 'iloc') else self.texts[idx])
        
        # í•œêµ­ì–´ ì „ì²˜ë¦¬
        text = self.preprocessor.clean_korean_text(text)
        
        # ë°ì´í„° ì¦ê°• (í›ˆë ¨ì‹œì—ë§Œ)
        if self.augment:
            text = self._simple_augment(text)
        
        # í•œêµ­ì–´ íŠ¹ì§• ì¶”ì¶œ
        korean_features = self.preprocessor.extract_korean_features(text)
        
        # ì²­í‚¹
        chunks = self._smart_korean_chunk(text)
        
        # ê° ì²­í¬ ì¸ì½”ë”©
        chunk_encodings = []
        for chunk in chunks:
            encoding = self.tokenizer(
                chunk,
                add_special_tokens=True,
                max_length=self.max_len,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
            
            chunk_encodings.append({
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten()
            })
        
        # íŒ¨ë”©
        while len(chunk_encodings) < MAX_CHUNKS:
            chunk_encodings.append({
                'input_ids': torch.zeros(self.max_len, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_len, dtype=torch.long)
            })
        
        # í•œêµ­ì–´ íŠ¹ì§•ì„ í…ì„œë¡œ ë³€í™˜
        feature_tensor = torch.tensor([
            korean_features.get('korean_ratio', 0),
            korean_features.get('particle_density', 0),
            korean_features.get('formal_ratio', 0),
            korean_features.get('punctuation_variety', 0),
            len(text) / 1000,  # ì •ê·œí™”ëœ ê¸¸ì´
        ], dtype=torch.float32)
        
        result = {
            'input_ids': torch.stack([chunk['input_ids'] for chunk in chunk_encodings]),
            'attention_mask': torch.stack([chunk['attention_mask'] for chunk in chunk_encodings]),
            'num_chunks': torch.tensor(min(len(chunks), MAX_CHUNKS), dtype=torch.long),
            'korean_features': feature_tensor
        }
        
        if self.labels is not None:
            label_value = self.labels.iloc[idx] if hasattr(self.labels, 'iloc') else self.labels[idx]
            result['labels'] = torch.tensor(label_value, dtype=torch.long)
            
        return result

# ================================================================
# í•œêµ­ì–´ íŠ¹í™” KoBERT ëª¨ë¸
# ================================================================

class EnhancedKoBERTClassifier(nn.Module):
    """í•œêµ­ì–´ íŠ¹í™” í–¥ìƒëœ KoBERT ë¶„ë¥˜ê¸°"""
    
    def __init__(self, model_name=MODEL_NAME, num_classes=2):
        super(EnhancedKoBERTClassifier, self).__init__()
        
        self.bert = AutoModel.from_pretrained(model_name)
        
        # KoBERT íŠ¹ì„±ì— ë§ëŠ” ë ˆì´ì–´ ê³ ì • (ëœ ê³µê²©ì ìœ¼ë¡œ)
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:4]:  # ì²˜ìŒ 4ê°œë§Œ ê³ ì •
            for param in layer.parameters():
                param.requires_grad = False
        
        hidden_size = self.bert.config.hidden_size
        
        # ìœ„ì¹˜ ì¸ì½”ë”© (í•œêµ­ì–´ ì–´ìˆœ ê³ ë ¤)
        self.position_embeddings = nn.Embedding(MAX_CHUNKS, hidden_size)
        
        # ì²­í¬ ê°„ ì–´í…ì…˜ (í•œêµ­ì–´ ë¬¸ë§¥ ê³ ë ¤)
        self.chunk_attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, batch_first=True, dropout=DROPOUT_RATE
        )
        
        # í•œêµ­ì–´ íŠ¹ì§• ìœµí•© ë„¤íŠ¸ì›Œí¬
        self.feature_fusion = nn.Sequential(
            nn.Linear(5, hidden_size // 8),  # í•œêµ­ì–´ íŠ¹ì§• 5ê°œ
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE // 2)
        )
        
        # ì²­í¬ ê°€ì¤‘ì¹˜ ê³„ì‚° (í•œêµ­ì–´ íŠ¹ì„± ë°˜ì˜)
        self.chunk_weight_net = nn.Sequential(
            nn.Linear(hidden_size + hidden_size // 8, hidden_size // 4),
            nn.GELU(),  # KoBERTì™€ í˜¸í™˜ì„± ì¢‹ì€ í™œì„±í™” í•¨ìˆ˜
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(hidden_size // 4, 1)
        )
        
        # í–¥ìƒëœ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(hidden_size + hidden_size // 8, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.GELU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.GELU(),
            nn.Dropout(DROPOUT_RATE // 2),
            nn.Linear(hidden_size // 4, num_classes)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
        
    def _init_weights(self):
        """KoBERTì— ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in [self.feature_fusion, self.chunk_weight_net, self.classifier]:
            for m in module.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif isinstance(m, nn.LayerNorm):
                    nn.init.ones_(m.weight)
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_ids, attention_mask, num_chunks, korean_features):
        batch_size, num_chunks_max, seq_len = input_ids.shape
        
        # KoBERT ì¸ì½”ë”©
        input_ids_flat = input_ids.view(-1, seq_len)
        attention_mask_flat = attention_mask.view(-1, seq_len)
        
        outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
        
        # KoBERTëŠ” pooler_outputì´ ì—†ì„ ìˆ˜ ìˆìŒ
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            chunk_embeddings = outputs.pooler_output
        else:
            chunk_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°
        
        chunk_embeddings = chunk_embeddings.view(batch_size, num_chunks_max, -1)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        positions = torch.arange(num_chunks_max, device=chunk_embeddings.device).unsqueeze(0).repeat(batch_size, 1)
        position_embeddings = self.position_embeddings(positions)
        chunk_embeddings = chunk_embeddings + position_embeddings
        
        # í•œêµ­ì–´ íŠ¹ì§• ì²˜ë¦¬
        korean_feat_embedding = self.feature_fusion(korean_features)
        korean_feat_expanded = korean_feat_embedding.unsqueeze(1).repeat(1, num_chunks_max, 1)
        
        # íŠ¹ì§• ìœµí•©
        enhanced_chunks = torch.cat([chunk_embeddings, korean_feat_expanded], dim=-1)
        
        # ì²­í¬ ë§ˆìŠ¤í¬ ìƒì„±
        chunk_mask = torch.zeros(batch_size, num_chunks_max, device=chunk_embeddings.device)
        for i, num_chunk in enumerate(num_chunks):
            chunk_mask[i, :num_chunk] = 1
        
        # ì–´í…ì…˜ ì ìš©
        attended_chunks, _ = self.chunk_attention(
            chunk_embeddings, chunk_embeddings, chunk_embeddings,
            key_padding_mask=(chunk_mask == 0)
        )
        
        # ì–´í…ì…˜ ê²°ê³¼ì™€ í•œêµ­ì–´ íŠ¹ì§• ê²°í•©
        attended_enhanced = torch.cat([attended_chunks, korean_feat_expanded], dim=-1)
        
        # ì²­í¬ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        chunk_weights = self.chunk_weight_net(attended_enhanced).squeeze(-1)
        chunk_weights = chunk_weights.masked_fill(chunk_mask == 0, float('-inf'))
        chunk_weights = F.softmax(chunk_weights, dim=1)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë¬¸ì„œ í‘œí˜„ ìƒì„±
        doc_embedding = torch.sum(attended_enhanced * chunk_weights.unsqueeze(-1), dim=1)
        
        # ë¶„ë¥˜
        logits = self.classifier(doc_embedding)
        
        return logits

# ================================================================
# í›ˆë ¨ í•¨ìˆ˜
# ================================================================

def train_enhanced_kobert(model, X, y, tokenizer):
    """í–¥ìƒëœ KoBERT í›ˆë ¨"""
    
    # ë°ì´í„° ë¶„í•  (ë” ì‹ ì¤‘í•˜ê²Œ)
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, stratify=y, test_size=0.25, random_state=42
    )
    
    print(f"ğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"   Train: {len(X_train)} ({dict(y_train.value_counts())})")
    print(f"   Val: {len(X_val)} ({dict(y_val.value_counts())})")
    
    # ë°ì´í„°ì…‹ ìƒì„± (í›ˆë ¨ìš©ì€ ì¦ê°• ì ìš©)
    train_dataset = OptimizedKoreanDataset(X_train, y_train, tokenizer, augment=True)
    val_dataset = OptimizedKoreanDataset(X_val, y_val, tokenizer, augment=False)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
    
    # ì°¨ë³„ì  í•™ìŠµë¥  ì ìš©
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.bert.named_parameters() if not any(nd in n for nd in no_decay)],
            'weight_decay': WEIGHT_DECAY,
            'lr': LEARNING_RATE * 0.1  # BERTëŠ” ì‘ì€ í•™ìŠµë¥ 
        },
        {
            'params': [p for n, p in model.bert.named_parameters() if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': LEARNING_RATE * 0.1
        },
        {
            'params': [p for n, p in model.named_parameters() if 'bert' not in n and not any(nd in n for nd in no_decay)],
            'weight_decay': WEIGHT_DECAY,
            'lr': LEARNING_RATE
        },
        {
            'params': [p for n, p in model.named_parameters() if 'bert' not in n and any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': LEARNING_RATE
        }
    ]
    
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
    
    # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ (ë” ë¶€ë“œëŸ¬ìš´ í•™ìŠµë¥  ê°ì†Œ)
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
    )
    
    # Label smoothingì´ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜
    class LabelSmoothingCrossEntropy(nn.Module):
        def __init__(self, smoothing=0.1):
            super().__init__()
            self.smoothing = smoothing
            
        def forward(self, pred, target):
            confidence = 1.0 - self.smoothing
            log_probs = F.log_softmax(pred, dim=1)
            smooth_target = target * confidence + (1 - target) * self.smoothing / (pred.size(1) - 1)
            return F.nll_loss(log_probs, target)
    
    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
    
    # Mixed precision
    scaler = torch.cuda.amp.GradScaler()
    
    best_auc = 0
    best_model_state = None
    patience = 0
    max_patience = 2
    
    print("ğŸš€ í–¥ìƒëœ KoBERT í›ˆë ¨ ì‹œì‘!")
    print("=" * 70)
    
    for epoch in range(EPOCHS):
        # === í›ˆë ¨ ===
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]')
        
        for batch in train_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            korean_features = batch['korean_features'].to(device)
            
            optimizer.zero_grad()
            
            # Forward pass with mixed precision
            with torch.cuda.amp.autocast():
                logits = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    num_chunks=num_chunks,
                    korean_features=korean_features
                )
                loss = criterion(logits, labels)
            
            # Backward pass
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            
            # í†µê³„
            total_loss += loss.item()
            _, preds = torch.max(logits, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            # Progress bar ì—…ë°ì´íŠ¸
            train_bar.set_postfix({
                'Loss': f'{total_loss/(train_bar.n+1):.4f}',
                'Acc': f'{100*correct/total:.1f}%',
                'LR': f'{scheduler.get_last_lr()[0]:.2e}'
            })
        
        train_loss = total_loss / len(train_loader)
        train_acc = correct / total
        
        # === ê²€ì¦ ===
        model.eval()
        val_predictions = []
        val_labels_list = []
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]')
            
            for batch in val_bar:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                num_chunks = batch['num_chunks'].to(device)
                korean_features = batch['korean_features'].to(device)
                
                with torch.cuda.amp.autocast():
                    logits = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        num_chunks=num_chunks,
                        korean_features=korean_features
                    )
                    loss = criterion(logits, labels)
                
                val_loss += loss.item()
                _, preds = torch.max(logits, 1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)
                
                probs = torch.softmax(logits, dim=1)
                val_predictions.extend(probs[:, 1].cpu().numpy())
                val_labels_list.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = val_correct / val_total
        val_auc = roc_auc_score(val_labels_list, val_predictions)
        val_f1 = f1_score(val_labels_list, [1 if p > 0.5 else 0 for p in val_predictions])
        val_precision = precision_score(val_labels_list, [1 if p > 0.5 else 0 for p in val_predictions])
        val_recall = recall_score(val_labels_list, [1 if p > 0.5 else 0 for p in val_predictions])
        
        # ìƒì„¸í•œ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š Epoch {epoch+1} ìƒì„¸ ê²°ê³¼:")
        print(f"   ğŸ¯ Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f}")
        print(f"   ğŸ¯ Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f}")
        print(f"   ğŸ“ˆ Metrics - AUC: {val_auc:.4f} | F1: {val_f1:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_auc > best_auc:
            best_auc = val_auc
            best_model_state = model.state_dict().copy()
            patience = 0
            
            # ëª¨ë¸ ì €ì¥
            model_save_path = f'{MODEL_PATH}/enhanced_kobert_best.pth'
            torch.save({
                'model_state_dict': best_model_state,
                'val_auc': best_auc,
                'epoch': epoch,
                'hyperparameters': {
                    'max_len': MAX_LEN,
                    'batch_size': BATCH_SIZE,
                    'learning_rate': LEARNING_RATE,
                    'model_name': MODEL_NAME
                }
            }, model_save_path)
            
            print(f"   â­ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! AUC: {val_auc:.4f} (ëª¨ë¸ ì €ì¥ë¨)")
        else:
            patience += 1
            print(f"   â³ Patience: {patience}/{max_patience}")
            
            if patience >= max_patience:
                print(f"   â° Early stopping!")
                break
        
        print("-" * 70)
        torch.cuda.empty_cache()
        gc.collect()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    return model, best_auc

def predict_enhanced_model(model, data_loader):
    """í–¥ìƒëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc='ğŸ”® ì •êµí•œ ì˜ˆì¸¡ ì¤‘'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            korean_features = batch['korean_features'].to(device)
            
            with torch.cuda.amp.autocast():
                logits = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    num_chunks=num_chunks,
                    korean_features=korean_features
                )
            
            probs = torch.softmax(logits, dim=1)
            predictions.extend(probs[:, 1].cpu().numpy())
    
    return np.array(predictions)

# ================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ================================================================

def main():
    print("ğŸ¯ ì •êµí•œ KoBERT Fine-tuning (í•œêµ­ì–´ íŠ¹í™”)")
    print("=" * 70)
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“ ë°ì´í„° ë¡œë”©...")
    train = pd.read_csv(f'{DATA_PATH}/train.csv', encoding='utf-8-sig')
    test = pd.read_csv(f'{DATA_PATH}/test.csv', encoding='utf-8-sig')
    
    print(f"ğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"   Train: {train.shape}")
    print(f"   Test: {test.shape}")
    print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(train['generated'].value_counts())}")
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    train_lengths = train['full_text'].astype(str).apply(len)
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´ - í‰ê· : {train_lengths.mean():.0f}, ì¤‘ì•™ê°’: {train_lengths.median():.0f}, ìµœëŒ€: {train_lengths.max():,}")
    
    # í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬
    print("\nğŸ”§ í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬...")
    preprocessor = KoreanTextPreprocessor()
    
    # Train ë°ì´í„°
    train['title'] = train['title'].fillna('').astype(str).apply(preprocessor.clean_korean_text)
    train['full_text'] = train['full_text'].fillna('').astype(str).apply(preprocessor.clean_korean_text)
    
    # ì œëª©ê³¼ ë³¸ë¬¸ ê²°í•© (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)
    train['combined_text'] = train.apply(
        lambda x: f"{x['title']}\n\n{x['full_text']}" if x['title'] and x['full_text'] 
        else x['title'] or x['full_text'], axis=1
    )
    
    # Test ë°ì´í„°
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('').astype(str).apply(preprocessor.clean_korean_text)
    test['full_text'] = test['full_text'].fillna('').astype(str).apply(preprocessor.clean_korean_text)
    test['combined_text'] = test.apply(
        lambda x: f"{x['title']}\n\n{x['full_text']}" if x['title'] and x['full_text'] 
        else x['title'] or x['full_text'], axis=1
    )
    
    X = train['combined_text']
    y = train['generated']
    
    # í•œêµ­ì–´ íŠ¹ì„± ë¶„ì„
    korean_stats = X.apply(preprocessor.extract_korean_features)
    avg_korean_ratio = np.mean([stats.get('korean_ratio', 0) for stats in korean_stats])
    print(f"   í‰ê·  í•œêµ­ì–´ ë¹„ìœ¨: {avg_korean_ratio:.2f}")
    
    # KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ¤– KoBERT í† í¬ë‚˜ì´ì € ë¡œë”©: {MODEL_NAME}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        print("   âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"   âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
    test_tokens = tokenizer.encode(test_text)
    print(f"   í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸: {len(test_tokens)}ê°œ í† í°")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ—ï¸ í–¥ìƒëœ KoBERT ëª¨ë¸ ì´ˆê¸°í™”...")
    model = EnhancedKoBERTClassifier(MODEL_NAME)
    model = model.to(device)
    
    # ëª¨ë¸ ì •ë³´
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
    print(f"      ì „ì²´: {total_params:,}")
    print(f"      í›ˆë ¨ê°€ëŠ¥: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    # í›ˆë ¨
    print("\nğŸš€ ì •êµí•œ í›ˆë ¨ ì‹œì‘...")
    trained_model, best_auc = train_enhanced_kobert(model, X, y, tokenizer)
    
    print(f"\nğŸ† í›ˆë ¨ ì™„ë£Œ!")
    print(f"   ìµœê³  ê²€ì¦ AUC: {best_auc:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...")
    test_dataset = OptimizedKoreanDataset(
        test['combined_text'], labels=None, tokenizer=tokenizer, augment=False
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        num_workers=2, 
        pin_memory=True
    )
    
    predictions = predict_enhanced_model(trained_model, test_loader)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¶„ì„
    print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¶„ì„:")
    print(f"   ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions):,}")
    print(f"   í‰ê·  í™•ë¥ : {predictions.mean():.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {predictions.std():.4f}")
    print(f"   ìµœì†Œê°’: {predictions.min():.4f}")
    print(f"   ìµœëŒ€ê°’: {predictions.max():.4f}")
    print(f"   ì¤‘ì•™ê°’: {np.median(predictions):.4f}")
    
    # ë¶„í¬ ë¶„ì„
    high_conf = np.sum(predictions > 0.8)
    medium_conf = np.sum((predictions >= 0.2) & (predictions <= 0.8))
    low_conf = np.sum(predictions < 0.2)
    
    print(f"   ì‹ ë¢°ë„ ë¶„í¬:")
    print(f"      ê³ ì‹ ë¢°ë„ (>0.8): {high_conf} ({high_conf/len(predictions)*100:.1f}%)")
    print(f"      ì¤‘ê°„ì‹ ë¢°ë„ (0.2-0.8): {medium_conf} ({medium_conf/len(predictions)*100:.1f}%)")
    print(f"      ì €ì‹ ë¢°ë„ (<0.2): {low_conf} ({low_conf/len(predictions)*100:.1f}%)")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    print("\nğŸ’¾ ì œì¶œ íŒŒì¼ ìƒì„±...")
    sample_submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = predictions
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ëª…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_path = f'{RESULT_PATH}/enhanced_kobert_submission_{timestamp}.csv'
    sample_submission.to_csv(submission_path, index=False)
    
    # ê¸°ë³¸ submission íŒŒì¼
    baseline_path = f'{RESULT_PATH}/baseline_submission.csv'
    sample_submission.to_csv(baseline_path, index=False)
    
    print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ:")
    print(f"   ğŸ“ ìƒì„¸ë²„ì „: {submission_path}")
    print(f"   ğŸ“ ê¸°ë³¸ë²„ì „: {baseline_path}")
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ‰ ì •êµí•œ KoBERT Fine-tuning ì™„ë£Œ!")
    print("="*70)
    print(f"ğŸ† ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   ìµœê³  ê²€ì¦ AUC: {best_auc:.4f}")
    print(f"   ì˜ˆì¸¡ í‰ê·  í™•ë¥ : {predictions.mean():.4f}")
    print(f"   ì˜ˆì¸¡ ì‹ ë¢°ë„: {predictions.std():.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    
    print(f"\nğŸ¯ í•œêµ­ì–´ íŠ¹í™” ê°œì„ ì‚¬í•­:")
    print(f"   âœ… í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡° ì¸ì‹")
    print(f"   âœ… ì¡°ì‚¬/ì–´ë¯¸ íŒ¨í„´ ë¶„ì„")
    print(f"   âœ… í•œêµ­ì–´ íŠ¹ì§• ìœµí•©")
    print(f"   âœ… ë°ì´í„° ì¦ê°• ì ìš©")
    print(f"   âœ… ì°¨ë³„ì  í•™ìŠµë¥ ")
    print(f"   âœ… Label smoothing")
    
    print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"   ğŸ¤– ëª¨ë¸: {MODEL_PATH}/enhanced_kobert_best.pth")
    print(f"   ğŸ“ ì œì¶œ: {submission_path}")
    print("="*70)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()
    
    return trained_model, best_auc, predictions

# ================================================================
# ì‹¤í–‰
# ================================================================

if __name__ == "__main__":
    try:
        model, auc, preds = main()
        print(f"\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"   ìµœì¢… AUC: {auc:.4f}")
        print(f"   ì˜ˆì¸¡ íŒŒì¼ì´ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
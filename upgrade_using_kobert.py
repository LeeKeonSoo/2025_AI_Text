import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
from tqdm import tqdm
import warnings
import random
import os
warnings.filterwarnings('ignore')

# ì‹œë“œ ê³ ì • (ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼)
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# ê³ ê¸‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
MAX_LEN = 512
BATCH_SIZE = 12  # ì¡°ê¸ˆ ì¤„ì—¬ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ
EPOCHS = 5  # ë” ë§ì€ ì—í¬í¬ë¡œ ê¹Šì´ í•™ìŠµ
LEARNING_RATE = 1e-5  # ë” ë‚®ì€ í•™ìŠµë¥ ë¡œ ì •êµí•œ í•™ìŠµ
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01
model_name = 'skt/kobert-base-v1'

# ê³ ê¸‰ ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì „ëµ
CHUNK_SIZE = 350
OVERLAP_SIZE = 75
MAX_CHUNKS = 4  # ë” ë§ì€ ì •ë³´ í™œìš©
USE_KFOLD = False  # K-Fold êµì°¨ ê²€ì¦ ì‚¬ìš© ì—¬ë¶€
N_FOLDS = 5

# ê³ ê¸‰ í•™ìŠµ ê¸°ë²•
USE_FOCAL_LOSS = True  # ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘
USE_LABEL_SMOOTHING = True  # ê³¼ì í•© ë°©ì§€
USE_ADVERSARIAL_TRAINING = True  # ê²¬ê³ ì„± í–¥ìƒ
USE_ENSEMBLE = True  # ì•™ìƒë¸” í•™ìŠµ

class AdvancedLongTextDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, max_len=MAX_LEN, augment=False):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augment = augment
        
    def __len__(self):
        return len(self.texts)
    
    def _augment_text(self, text):
        """ë°ì´í„° ì¦ê°• (í›ˆë ¨ ì‹œë§Œ)"""
        if not self.augment or random.random() > 0.3:
            return text
            
        # ê°„ë‹¨í•œ ì¦ê°• ê¸°ë²•ë“¤
        augmentation_type = random.choice(['none', 'shuffle_sentences', 'drop_sentences'])
        
        if augmentation_type == 'shuffle_sentences':
            sentences = text.split('. ')
            if len(sentences) > 3:
                # ë¬¸ì¥ ìˆœì„œ ì¼ë¶€ ë³€ê²½
                mid_idx = len(sentences) // 2
                random.shuffle(sentences[1:mid_idx])
                text = '. '.join(sentences)
        
        elif augmentation_type == 'drop_sentences':
            sentences = text.split('. ')
            if len(sentences) > 5:
                # ì¼ë¶€ ë¬¸ì¥ ì œê±° (ìµœëŒ€ 20%)
                drop_count = min(len(sentences) // 5, 2)
                indices_to_drop = random.sample(range(1, len(sentences)-1), drop_count)
                sentences = [s for i, s in enumerate(sentences) if i not in indices_to_drop]
                text = '. '.join(sentences)
        
        return text
    
    def _chunk_text_advanced(self, text):
        """ê³ ê¸‰ ì²­í‚¹ ì „ëµ"""
        # ë°ì´í„° ì¦ê°• ì ìš©
        text = self._augment_text(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        sentences = text.split('. ')
        if len(sentences) == 1:
            sentences = text.split('.')
        
        # í† í° ê¸°ë°˜ ì²­í‚¹
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        
        if len(tokens) <= self.max_len - 2:
            return [text]
        
        chunks = []
        current_chunk_tokens = []
        current_length = 0
        
        # ë¬¸ì¥ë³„ë¡œ ì²­í¬ì— ì¶”ê°€
        for sentence in sentences:
            sentence_tokens = self.tokenizer.encode(sentence + '.', add_special_tokens=False)
            
            if current_length + len(sentence_tokens) > CHUNK_SIZE:
                if current_chunk_tokens:
                    chunk_text = self.tokenizer.decode(current_chunk_tokens, skip_special_tokens=True)
                    chunks.append(chunk_text)
                    # ì˜¤ë²„ë© ìœ ì§€
                    overlap_tokens = current_chunk_tokens[-OVERLAP_SIZE:] if len(current_chunk_tokens) > OVERLAP_SIZE else []
                    current_chunk_tokens = overlap_tokens + sentence_tokens
                    current_length = len(current_chunk_tokens)
                else:
                    current_chunk_tokens = sentence_tokens
                    current_length = len(sentence_tokens)
            else:
                current_chunk_tokens.extend(sentence_tokens)
                current_length += len(sentence_tokens)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk_tokens:
            chunk_text = self.tokenizer.decode(current_chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
        
        return chunks
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        
        # ê³ ê¸‰ ì²­í‚¹
        chunks = self._chunk_text_advanced(text)
        
        # ê° ì²­í¬ë¥¼ í† í¬ë‚˜ì´ì§•
        chunk_encodings = []
        for chunk in chunks:
            encoding = self.tokenizer(
                chunk,
                add_special_tokens=True,
                max_length=self.max_len,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
            chunk_encodings.append({
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten()
            })
        
        # íŒ¨ë”©
        while len(chunk_encodings) < MAX_CHUNKS:
            chunk_encodings.append({
                'input_ids': torch.zeros(self.max_len, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_len, dtype=torch.long)
            })
        
        chunk_encodings = chunk_encodings[:MAX_CHUNKS]
        
        result = {
            'input_ids': torch.stack([chunk['input_ids'] for chunk in chunk_encodings]),
            'attention_mask': torch.stack([chunk['attention_mask'] for chunk in chunk_encodings]),
            'num_chunks': torch.tensor(min(len(chunks), MAX_CHUNKS), dtype=torch.long)
        }
        
        if self.labels is not None:
            result['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
            
        return result

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing
    
    def forward(self, x, target):
        confidence = 1. - self.smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()

class AdvancedKoBERTClassifier(nn.Module):
    def __init__(self, model_name, num_classes=2, dropout_rate=0.15):
        super(AdvancedKoBERTClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        
        # ì„ íƒì  ë ˆì´ì–´ í”„ë¦¬ì§• (ì²˜ìŒ 6ê°œë§Œ)
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:6]:
            for param in layer.parameters():
                param.requires_grad = False
        
        hidden_size = self.bert.config.hidden_size
        
        # ê³ ê¸‰ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.chunk_attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, batch_first=True, dropout=dropout_rate
        )
        
        # ìœ„ì¹˜ ì¸ì½”ë”© (ì²­í¬ ìœ„ì¹˜ ì •ë³´)
        self.position_embeddings = nn.Embedding(MAX_CHUNKS, hidden_size)
        
        # ê³ ê¸‰ ë¶„ë¥˜ê¸° (ì”ì°¨ ì—°ê²° í¬í•¨)
        self.pre_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, input_ids, attention_mask, num_chunks):
        batch_size, num_chunks_max, seq_len = input_ids.shape
        
        with torch.cuda.amp.autocast():
            # ëª¨ë“  ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            input_ids_flat = input_ids.view(-1, seq_len)
            attention_mask_flat = attention_mask.view(-1, seq_len)
            
            # BERT ì¸ì½”ë”©
            outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
            
            # CLS í† í° ì¶”ì¶œ
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                chunk_embeddings = outputs.pooler_output
            else:
                chunk_embeddings = outputs.last_hidden_state[:, 0, :]
            
            # ì²­í¬ ì„ë² ë”©ì„ ë‹¤ì‹œ ë°°ì¹˜ í˜•íƒœë¡œ ë³€í™˜
            chunk_embeddings = chunk_embeddings.view(batch_size, num_chunks_max, -1)
            
            # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
            positions = torch.arange(num_chunks_max, device=chunk_embeddings.device).unsqueeze(0).repeat(batch_size, 1)
            position_embeddings = self.position_embeddings(positions)
            chunk_embeddings = chunk_embeddings + position_embeddings
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            chunk_mask = torch.zeros(batch_size, num_chunks_max, device=chunk_embeddings.device)
            for i, num_chunk in enumerate(num_chunks):
                chunk_mask[i, :num_chunk] = 1
            
            # ì–´í…ì…˜ ì ìš©
            attended_chunks, attention_weights = self.chunk_attention(
                chunk_embeddings, chunk_embeddings, chunk_embeddings,
                key_padding_mask=(chunk_mask == 0)
            )
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë¬¸ì„œ í‘œí˜„ ìƒì„±
            doc_mask = chunk_mask.unsqueeze(-1)
            weighted_chunks = attended_chunks * doc_mask
            doc_embedding = weighted_chunks.sum(dim=1) / (doc_mask.sum(dim=1) + 1e-8)
            
            # ì”ì°¨ ì—°ê²°
            doc_embedding = doc_embedding + self.pre_classifier(doc_embedding)
            doc_embedding = self.dropout(doc_embedding)
            
            # ë¶„ë¥˜
            logits = self.classifier(doc_embedding)
        
        return logits, attention_weights

def adversarial_training(model, inputs, labels, optimizer, criterion, epsilon=0.01):
    """ì ëŒ€ì  í›ˆë ¨"""
    # ì›ë³¸ ì„ë² ë”© ì €ì¥
    embeddings = model.bert.embeddings.word_embeddings
    original_embeddings = embeddings.weight.data.clone()
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    embeddings.weight.requires_grad_()
    
    with torch.cuda.amp.autocast():
        logits, _ = model(**inputs)
        loss = criterion(logits, labels)
    
    # ì„ë² ë”©ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸
    loss.backward(retain_graph=True)
    grad = embeddings.weight.grad.data
    
    # ì ëŒ€ì  perturbation ì¶”ê°€
    perturbation = epsilon * grad.sign()
    embeddings.weight.data = original_embeddings + perturbation
    
    # ì ëŒ€ì  ìƒ˜í”Œë¡œ ë‹¤ì‹œ ê³„ì‚°
    with torch.cuda.amp.autocast():
        adv_logits, _ = model(**inputs)
        adv_loss = criterion(adv_logits, labels)
    
    # ì›ë³¸ ì„ë² ë”© ë³µì›
    embeddings.weight.data = original_embeddings
    embeddings.weight.requires_grad_(False)
    
    return adv_loss

def train_epoch_advanced(model, data_loader, optimizer, criterion, scheduler, device, scaler, epoch):
    model.train()
    total_loss = 0
    correct_predictions = 0
    
    for batch_idx, batch in enumerate(tqdm(data_loader, desc=f'Training Epoch {epoch}')):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        num_chunks = batch['num_chunks'].to(device)
        
        optimizer.zero_grad()
        
        inputs = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'num_chunks': num_chunks
        }
        
        # ì¼ë°˜ í›ˆë ¨
        with torch.cuda.amp.autocast():
            logits, attention_weights = model(**inputs)
            loss = criterion(logits, labels)
        
        # ì ëŒ€ì  í›ˆë ¨ (ì¼ë¶€ ë°°ì¹˜ì—ì„œë§Œ)
        if USE_ADVERSARIAL_TRAINING and batch_idx % 3 == 0:
            adv_loss = adversarial_training(model, inputs, labels, optimizer, criterion)
            loss = 0.7 * loss + 0.3 * adv_loss
        
        _, preds = torch.max(logits, dim=1)
        correct_predictions += torch.sum(preds == labels)
        total_loss += loss.item()
        
        # ìŠ¤ì¼€ì¼ëœ ì—­ì „íŒŒ
        scaler.scale(loss).backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
    
    return total_loss / len(data_loader), correct_predictions.double() / len(data_loader.dataset)

def eval_model_advanced(model, data_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    predictions = []
    probabilities = []
    real_values = []
    attention_scores = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Evaluating'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            
            with torch.cuda.amp.autocast():
                logits, attention_weights = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    num_chunks=num_chunks
                )
                loss = criterion(logits, labels)
            
            _, preds = torch.max(logits, dim=1)
            correct_predictions += torch.sum(preds == labels)
            total_loss += loss.item()
            
            # í™•ë¥  ê³„ì‚°
            probs = torch.softmax(logits, dim=1)
            
            predictions.extend(preds.cpu().numpy())
            probabilities.extend(probs[:, 1].cpu().numpy())
            real_values.extend(labels.cpu().numpy())
            
            # ì–´í…ì…˜ ì ìˆ˜ ì €ì¥ (ë¶„ì„ìš©)
            attention_scores.extend(attention_weights.mean(dim=1).cpu().numpy())
    
    # F1 ì ìˆ˜ ê³„ì‚°
    f1 = f1_score(real_values, predictions)
    
    return (total_loss / len(data_loader), 
            correct_predictions.double() / len(data_loader.dataset),
            predictions, probabilities, real_values, f1, attention_scores)

def predict_test_advanced(model, data_loader, device):
    model.eval()
    test_predictions = []
    test_probabilities = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Predicting'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            num_chunks = batch['num_chunks'].to(device)
            
            with torch.cuda.amp.autocast():
                logits, _ = model(
                    input_ids=input_ids, 
                    attention_mask=attention_mask, 
                    num_chunks=num_chunks
                )
            
            probs = torch.softmax(logits, dim=1)
            _, preds = torch.max(logits, dim=1)
            
            test_predictions.extend(preds.cpu().numpy())
            test_probabilities.extend(probs[:, 1].cpu().numpy())
    
    return test_predictions, test_probabilities

def main():
    print("=== Advanced KoBERT Training ===")
    print("Loading data...")
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    print("Advanced preprocessing...")
    
    train['title'] = train['title'].fillna('').str.strip()
    train['full_text'] = train['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    train['combined_text'] = train['title'] + ' [SEP] ' + train['full_text']
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    text_lengths = train['combined_text'].str.len()
    print(f"Text length analysis:")
    print(f"  Mean: {text_lengths.mean():.1f}")
    print(f"  Median: {text_lengths.median():.1f}")
    print(f"  95th percentile: {text_lengths.quantile(0.95):.1f}")
    print(f"  Max: {text_lengths.max()}")
    print(f"  Texts > 2000 chars: {(text_lengths > 2000).sum()}/{len(train)} ({(text_lengths > 2000).mean()*100:.1f}%)")
    
    X = train['combined_text']
    y = train['generated']
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
    class_counts = y.value_counts()
    print(f"Class distribution: {class_counts.to_dict()}")
    print(f"Class imbalance ratio: {class_counts[1] / class_counts[0]:.3f}")
    
    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
    
    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}")
    
    print("Loading advanced tokenizer and model...")
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        print(f"Successfully loaded tokenizer: {model_name}")
    except Exception as e:
        print(f"Failed to load KoBERT: {e}")
        model_name = 'klue/bert-base'
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print(f"Fallback to KLUE BERT: {model_name}")
    
    # ê³ ê¸‰ ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = AdvancedLongTextDataset(X_train, y_train, tokenizer, MAX_LEN, augment=True)
    val_dataset = AdvancedLongTextDataset(X_val, y_val, tokenizer, MAX_LEN, augment=False)
    
    # ê°€ì¤‘ ìƒ˜í”Œë§ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])
    weight = 1. / class_sample_count
    samples_weight = np.array([weight[t] for t in y_train])
    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))
    
    # ë°ì´í„°ë¡œë”
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, sampler=sampler,
        num_workers=4, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=4, pin_memory=True
    )
    
    print(f"Training batches: {len(train_loader)}")
    print(f"Validation batches: {len(val_loader)}")
    
    # ê³ ê¸‰ ëª¨ë¸ ì´ˆê¸°í™”
    model = AdvancedKoBERTClassifier(model_name, num_classes=2)
    model = model.to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # ê³ ê¸‰ ì†ì‹¤ í•¨ìˆ˜
    if USE_FOCAL_LOSS:
        criterion = FocalLoss(alpha=1, gamma=2)
        print("Using Focal Loss")
    elif USE_LABEL_SMOOTHING:
        criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
        print("Using Label Smoothing")
    else:
        criterion = nn.CrossEntropyLoss()
        print("Using Cross Entropy Loss")
    
    # í˜¼í•© ì •ë°€ë„ ìŠ¤ì¼€ì¼ëŸ¬
    scaler = torch.cuda.amp.GradScaler()
    
    # ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=LEARNING_RATE, 
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-6
    )
    
    # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"Training configuration:")
    print(f"  Epochs: {EPOCHS}")
    print(f"  Batch size: {BATCH_SIZE}")
    print(f"  Learning rate: {LEARNING_RATE}")
    print(f"  Total steps: {total_steps}")
    print(f"  Warmup steps: {warmup_steps}")
    print(f"  Advanced features: Focal Loss={USE_FOCAL_LOSS}, Label Smoothing={USE_LABEL_SMOOTHING}")
    print(f"  Adversarial Training={USE_ADVERSARIAL_TRAINING}")
    
    print("\nStarting advanced training...")
    
    # í›ˆë ¨ ì‹¤í–‰
    best_auc = 0
    best_f1 = 0
    best_model = None
    training_history = []
    
    for epoch in range(EPOCHS):
        print(f'\n=== Epoch {epoch + 1}/{EPOCHS} ===')
        
        # í›ˆë ¨
        train_loss, train_acc = train_epoch_advanced(
            model, train_loader, optimizer, criterion, scheduler, device, scaler, epoch + 1
        )
        
        # ê²€ì¦
        val_loss, val_acc, val_preds, val_probs, val_labels, val_f1, attention_scores = eval_model_advanced(
            model, val_loader, criterion, device
        )
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        val_auc = roc_auc_score(val_labels, val_probs)
        
        # í›ˆë ¨ ê¸°ë¡
        epoch_metrics = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc.item(),
            'val_loss': val_loss,
            'val_acc': val_acc.item(),
            'val_auc': val_auc,
            'val_f1': val_f1
        }
        training_history.append(epoch_metrics)
        
        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
        print(f'Val AUC: {val_auc:.4f}, Val F1: {val_f1:.4f}')
        
        # ëª¨ë¸ ì €ì¥ ì¡°ê±´ (AUCì™€ F1 ëª¨ë‘ ê³ ë ¤)
        combined_score = 0.7 * val_auc + 0.3 * val_f1
        best_combined = 0.7 * best_auc + 0.3 * best_f1
        
        if combined_score > best_combined:
            best_auc = val_auc
            best_f1 = val_f1
            best_model = model.state_dict().copy()
            print(f'ğŸ¯ New best model! AUC: {best_auc:.4f}, F1: {best_f1:.4f}')
        
        # í˜„ì¬ í•™ìŠµë¥  ì¶œë ¥
        current_lr = scheduler.get_last_lr()[0]
        print(f'Current LR: {current_lr:.2e}')
    
    print(f'\nğŸ† Best Results: AUC: {best_auc:.4f}, F1: {best_f1:.4f}')
    
    # ìµœê³  ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(best_model)
    model.eval()
    
    print("\nPreparing test data...")
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('').str.strip()
    test['full_text'] = test['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    test['combined_text'] = test['title'] + ' [SEP] ' + test['full_text']
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸¸ì´ ë¶„ì„
    test_lengths = test['combined_text'].str.len()
    print(f"Test text length stats:")
    print(f"  Mean: {test_lengths.mean():.1f}")
    print(f"  Max: {test_lengths.max()}")
    print(f"  Long texts (>2000): {(test_lengths > 2000).sum()}/{len(test)}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
    test_dataset = AdvancedLongTextDataset(test['combined_text'], labels=None, tokenizer=tokenizer, max_len=MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=4, pin_memory=True)
    
    print("Making predictions...")
    test_predictions, test_probabilities = predict_test_advanced(model, test_loader, device)
    
    print(f"Prediction stats:")
    print(f"  Generated {len(test_probabilities)} predictions")
    print(f"  Probability range: {min(test_probabilities):.4f} - {max(test_probabilities):.4f}")
    print(f"  Mean probability: {np.mean(test_probabilities):.4f}")
    print(f"  Std probability: {np.std(test_probabilities):.4f}")
    
    # ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„
    prob_bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    prob_hist, _ = np.histogram(test_probabilities, bins=prob_bins)
    print(f"  Prediction distribution:")
    for i in range(len(prob_bins)-1):
        print(f"    {prob_bins[i]:.1f}-{prob_bins[i+1]:.1f}: {prob_hist[i]}")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    sample_submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = test_probabilities
    
    # ì œì¶œ íŒŒì¼ ì €ì¥
    sample_submission.to_csv('./baseline_submission.csv', index=False)
    
    print(f"\nğŸ‰ Training completed successfully!")
    print(f"ğŸ“Š Final Results:")
    print(f"  Best Validation AUC: {best_auc:.4f}")
    print(f"  Best Validation F1: {best_f1:.4f}")
    print(f"  Model used: {model_name}")
    print(f"  Submission file: baseline_submission.csv")
    print(f"  Submission shape: {sample_submission.shape}")
    
    # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ìš”ì•½
    print(f"\nğŸ“ˆ Training History:")
    for i, history in enumerate(training_history):
        print(f"  Epoch {history['epoch']}: "
              f"Val AUC={history['val_auc']:.4f}, "
              f"Val F1={history['val_f1']:.4f}, "
              f"Val Acc={history['val_acc']:.4f}")
    
    print(sample_submission.head())

if __name__ == "__main__":
    main()
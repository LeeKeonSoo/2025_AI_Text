import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score
from tqdm import tqdm
import warnings
import random
import re
warnings.filterwarnings('ignore')

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True

set_seed(42)
device = torch.device('cuda')

# KoBERT ê·¹í•œ ìµœì í™” ì„¤ì •
MAX_LEN = 512
BATCH_SIZE = 16
EPOCHS = 6                    # ì¶©ë¶„í•œ í•™ìŠµ
LEARNING_RATE = 8e-6         # ì •êµí•œ í•™ìŠµë¥ 
WARMUP_RATIO = 0.15          # ê¸´ ì›Œë°ì—…
WEIGHT_DECAY = 0.01
DROPOUT_RATE = 0.15

# ê³ ê¸‰ ê¸°ë²• í™œìš©
USE_FOCAL_LOSS = True
USE_LABEL_SMOOTHING = True
USE_ADVERSARIAL = True
USE_MIXUP = True
USE_KFOLD = True
N_FOLDS = 5

print("ğŸ”¥ KoBERT ê·¹í•œ ìµœì í™” ëª¨ë“œ")

def advanced_text_preprocessing(text):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # ì—°ì†ëœ ë¬¸ì¥ë¶€í˜¸ ì •ë¦¬
    text = re.sub(r'[.]{2,}', '.', text)
    text = re.sub(r'[!]{2,}', '!', text)
    text = re.sub(r'[?]{2,}', '?', text)
    
    # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£.,!?;:()\[\]{}"\'-]', ' ', text)
    
    # ë‹¤ì‹œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

class AdvancedDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, is_train=False):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.is_train = is_train
        
    def __len__(self):
        return len(self.texts)
    
    def text_augmentation(self, text):
        """ë°ì´í„° ì¦ê°•"""
        if not self.is_train or random.random() > 0.3:
            return text
        
        sentences = text.split('. ')
        if len(sentences) > 3:
            # ë¬¸ì¥ ìˆœì„œ ì¼ë¶€ ì„ê¸°
            if random.random() < 0.5:
                mid = len(sentences) // 2
                random.shuffle(sentences[1:mid])
            
            # ì¼ë¶€ ë¬¸ì¥ ì œê±° (ìµœëŒ€ 20%)
            if random.random() < 0.3:
                remove_count = min(len(sentences) // 5, 2)
                if remove_count > 0:
                    indices = random.sample(range(1, len(sentences)-1), remove_count)
                    sentences = [s for i, s in enumerate(sentences) if i not in indices]
            
            text = '. '.join(sentences)
        
        return text
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        text = self.text_augmentation(text)
        
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=MAX_LEN,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        result = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        if self.labels is not None:
            result['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
            
        return result

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        return focal_loss

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, x, target):
        confidence = 1. - self.smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()

class ExtremeKoBERT(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = AutoModel.from_pretrained('skt/kobert-base-v1')
        
        # ìµœì†Œí•œì˜ í”„ë¦¬ì§• (ì„±ëŠ¥ ìµœìš°ì„ )
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:2]:  # ì²˜ìŒ 2ê°œë§Œ í”„ë¦¬ì§•
            for param in layer.parameters():
                param.requires_grad = False
        
        hidden_size = self.bert.config.hidden_size
        
        # ê³ ì„±ëŠ¥ ë¶„ë¥˜ í—¤ë“œ
        self.pre_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.Tanh(),
            nn.Dropout(DROPOUT_RATE)
        )
        
        # Multi-layer ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            
            nn.Linear(hidden_size, hidden_size // 2),
            nn.LayerNorm(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE // 2),
            
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE // 2),
            
            nn.Linear(hidden_size // 4, 2)
        )
        
        # ì–´í…ì…˜ í’€ë§
        self.attention_pool = nn.MultiheadAttention(
            hidden_size, num_heads=12, batch_first=True, dropout=DROPOUT_RATE
        )
        
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        # ê³ ê¸‰ í’€ë§: ì–´í…ì…˜ + CLS
        sequence_output = outputs.last_hidden_state
        
        # Self-attention pooling
        attended_output, _ = self.attention_pool(
            sequence_output, sequence_output, sequence_output,
            key_padding_mask=(attention_mask == 0)
        )
        
        # ê°€ì¤‘ í‰ê· 
        mask_expanded = attention_mask.unsqueeze(-1).float()
        weighted_output = (attended_output * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)
        
        # CLS í† í°ê³¼ ê²°í•©
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            cls_output = outputs.pooler_output
        else:
            cls_output = sequence_output[:, 0, :]
        
        # ë‘ í‘œí˜„ ê²°í•©
        combined = weighted_output + cls_output
        enhanced = self.pre_classifier(combined)
        enhanced = enhanced + combined  # ì”ì°¨ ì—°ê²°
        
        logits = self.classifier(enhanced)
        
        return logits

def mixup_data(x, y, alpha=0.2):
    """Mixup ë°ì´í„° ì¦ê°•"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def adversarial_training(model, inputs, labels, optimizer, criterion, epsilon=0.01):
    """ì ëŒ€ì  í›ˆë ¨"""
    embeddings = model.bert.embeddings.word_embeddings
    
    # ì›ë³¸ ì„ë² ë”© ì €ì¥
    original_embeddings = embeddings.weight.data.clone()
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ìœ„í•´ requires_grad ì„¤ì •
    embeddings.weight.requires_grad_()
    
    # Forward pass
    logits = model(inputs['input_ids'], inputs['attention_mask'])
    loss = criterion(logits, labels)
    
    # ì„ë² ë”©ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    loss.backward(retain_graph=True)
    grad = embeddings.weight.grad.data
    
    # ì ëŒ€ì  perturbation ìƒì„±
    perturbation = epsilon * grad.sign()
    embeddings.weight.data = original_embeddings + perturbation
    
    # ì ëŒ€ì  ìƒ˜í”Œë¡œ ë‹¤ì‹œ ê³„ì‚°
    adv_logits = model(inputs['input_ids'], inputs['attention_mask'])
    adv_loss = criterion(adv_logits, labels)
    
    # ì›ë³¸ ì„ë² ë”© ë³µì›
    embeddings.weight.data = original_embeddings
    embeddings.weight.requires_grad_(False)
    
    return adv_loss

def train_extreme_kobert(model, train_loader, val_loader, fold=None):
    # ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ì¡°í•©
    if USE_FOCAL_LOSS:
        criterion = FocalLoss(alpha=1, gamma=2)
    elif USE_LABEL_SMOOTHING:
        criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
    else:
        criterion = nn.CrossEntropyLoss()
    
    # ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=LEARNING_RATE, 
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-6
    )
    
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    
    # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    best_auc = 0
    best_state = None
    patience = 0
    max_patience = 3
    
    fold_str = f"Fold{fold}" if fold is not None else "Single"
    
    for epoch in range(EPOCHS):
        # í›ˆë ¨
        model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'{fold_str}-E{epoch+1}')):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}
            
            # Mixup ì ìš©
            if USE_MIXUP and random.random() < 0.3:
                mixed_inputs, y_a, y_b, lam = mixup_data(input_ids, labels)
                inputs['input_ids'] = mixed_inputs
                logits = model(**inputs)
                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
            else:
                logits = model(**inputs)
                loss = criterion(logits, labels)
            
            # ì ëŒ€ì  í›ˆë ¨ (ì¼ë¶€ ë°°ì¹˜ì—ì„œ)
            if USE_ADVERSARIAL and batch_idx % 3 == 0:
                adv_loss = adversarial_training(model, inputs, labels, optimizer, criterion)
                loss = 0.7 * loss + 0.3 * adv_loss
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_probs, val_labels = [], []
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                logits = model(input_ids, attention_mask)
                probs = torch.softmax(logits, dim=1)
                
                val_probs.extend(probs[:, 1].cpu().numpy())
                val_labels.extend(labels.cpu().numpy())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        auc = roc_auc_score(val_labels, val_probs)
        f1 = f1_score(val_labels, [1 if p > 0.5 else 0 for p in val_probs])
        acc = accuracy_score(val_labels, [1 if p > 0.5 else 0 for p in val_probs])
        
        avg_loss = total_loss / len(train_loader)
        current_lr = scheduler.get_last_lr()[0]
        
        print(f'  E{epoch+1}: Loss={avg_loss:.4f}, AUC={auc:.4f}, F1={f1:.4f}, Acc={acc:.4f}, LR={current_lr:.2e}')
        
        # Early stopping with patience
        if auc > best_auc:
            best_auc = auc
            best_state = model.state_dict().copy()
            patience = 0
            print(f'    ğŸš€ New best AUC: {best_auc:.4f}')
        else:
            patience += 1
            if patience >= max_patience:
                print(f'    Early stopping at epoch {epoch+1}')
                break
    
    model.load_state_dict(best_state)
    return model, best_auc

def predict_extreme(model, test_loader):
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc='Extreme Prediction'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            logits = model(input_ids, attention_mask)
            probs = torch.softmax(logits, dim=1)
            predictions.extend(probs[:, 1].cpu().numpy())
    
    return np.array(predictions)

def main():
    print("ğŸ”¥ KoBERT ê·¹í•œ ìµœì í™” ì‹œì‘!")
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    print(f"Train: {len(train)}, Test: {len(test)}")
    
    # ê³ ê¸‰ ì „ì²˜ë¦¬
    print("ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬...")
    train['title'] = train['title'].apply(advanced_text_preprocessing)
    train['full_text'] = train['full_text'].apply(advanced_text_preprocessing)
    train['text'] = train['title'] + ' [SEP] ' + train['full_text']
    
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].apply(advanced_text_preprocessing)
    test['full_text'] = test['full_text'].apply(advanced_text_preprocessing)
    test['text'] = test['title'] + ' [SEP] ' + test['full_text']
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    lengths = train['text'].str.len()
    print(f"í…ìŠ¤íŠ¸ ê¸¸ì´ - í‰ê· : {lengths.mean():.0f}, ìµœëŒ€: {lengths.max()}, 95%ile: {lengths.quantile(0.95):.0f}")
    
    X = train['text']
    y = train['generated']
    print(f"í´ë˜ìŠ¤ ë¶„í¬: {y.value_counts().to_dict()}")
    
    # KoBERT í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')
    
    if USE_KFOLD:
        # K-Fold êµì°¨ ê²€ì¦
        print(f"\n{N_FOLDS}-Fold êµì°¨ ê²€ì¦ ì‹œì‘")
        fold_predictions = []
        fold_aucs = []
        
        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"\n{'='*50}")
            print(f"Fold {fold+1}/{N_FOLDS}")
            print(f"{'='*50}")
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # ê°€ì¤‘ ìƒ˜í”Œë§
            class_counts = y_train.value_counts()
            class_weights = {0: 1.0, 1: class_counts[0] / class_counts[1]}
            sample_weights = y_train.map(class_weights)
            sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
            
            # ë°ì´í„°ì…‹
            train_dataset = AdvancedDataset(X_train, y_train, tokenizer, is_train=True)
            val_dataset = AdvancedDataset(X_val, y_val, tokenizer, is_train=False)
            
            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)
            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=4, pin_memory=True)
            
            # ëª¨ë¸ í›ˆë ¨
            model = ExtremeKoBERT().to(device)
            model, fold_auc = train_extreme_kobert(model, train_loader, val_loader, fold+1)
            
            fold_aucs.append(fold_auc)
            
            # ì²« ë²ˆì§¸ í´ë“œ ëª¨ë¸ ì €ì¥ (í…ŒìŠ¤íŠ¸ìš©)
            if fold == 0:
                best_model = model
        
        avg_auc = np.mean(fold_aucs)
        std_auc = np.std(fold_aucs)
        print(f"\nğŸ¯ K-Fold ê²°ê³¼: AUC = {avg_auc:.4f} Â± {std_auc:.4f}")
        print(f"Fold AUCs: {[f'{auc:.4f}' for auc in fold_aucs]}")
        
    else:
        # ë‹¨ì¼ í›ˆë ¨
        X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
        
        train_dataset = AdvancedDataset(X_train, y_train, tokenizer, is_train=True)
        val_dataset = AdvancedDataset(X_val, y_val, tokenizer, is_train=False)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=4, pin_memory=True)
        
        best_model = ExtremeKoBERT().to(device)
        best_model, best_auc = train_extreme_kobert(best_model, train_loader, val_loader)
        print(f"\nğŸ¯ ìµœì¢… AUC: {best_auc:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    print("\nğŸš€ ê·¹í•œ ì„±ëŠ¥ ì˜ˆì¸¡ ì‹œì‘...")
    test_dataset = AdvancedDataset(test['text'], labels=None, tokenizer=tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=4, pin_memory=True)
    
    predictions = predict_extreme(best_model, test_loader)
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
    print(f"  ë²”ìœ„: {predictions.min():.4f} - {predictions.max():.4f}")
    print(f"  í‰ê· : {predictions.mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {predictions.std():.4f}")
    
    # ì œì¶œ íŒŒì¼
    submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    submission['generated'] = predictions
    submission.to_csv('./extreme_kobert_submission.csv', index=False)
    
    print(f"\nğŸ”¥ KoBERT ê·¹í•œ ìµœì í™” ì™„ë£Œ!")
    print(f"ğŸ“ ì œì¶œ íŒŒì¼: extreme_kobert_submission.csv")
    print(f"ğŸ¯ ëª©í‘œ: AUC 0.90+ ë‹¬ì„±!")
    print(submission.head())

if __name__ == "__main__":
    main()
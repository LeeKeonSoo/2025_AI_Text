import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
from tqdm import tqdm
import warnings
import random
warnings.filterwarnings('ignore')

# ì‹œë“œ ê³ ì •
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name()}')
    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')

# A100 ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°
MAX_LEN = 1024  # ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (1024 ì‹œì‘, ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ 1536ê¹Œì§€)
BATCH_SIZE = 8  # A100ì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
EPOCHS = 4
LEARNING_RATE = 1e-5  # ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë” ë‚®ì€ í•™ìŠµë¥ 
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01

# ì•™ìƒë¸” ì„¤ì • (ì‹œê°„ ì ˆì•½)
ENSEMBLE_MODELS = [
    'skt/kobert-base-v1',
    'klue/bert-base'
]
USE_KFOLD = True
N_FOLDS = 2  # 2-Foldë¡œ ì‹œê°„ ì ˆì•½

class LongSequenceDataset(Dataset):
    """ê¸´ ì‹œí€€ìŠ¤ ì „ìš© ë°ì´í„°ì…‹ (ì²­í‚¹ ì—†ìŒ)"""
    def __init__(self, texts, labels=None, tokenizer=None, max_len=MAX_LEN):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        
        # ê¸´ ì‹œí€€ìŠ¤ ì§ì ‘ í† í¬ë‚˜ì´ì§• (ì²­í‚¹ ì—†ìŒ)
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,  # ë„ˆë¬´ ê¸´ ê²½ìš°ì—ë§Œ ìë¦„
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        result = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        if self.labels is not None:
            result['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
            
        return result

class LongSequenceBERTClassifier(nn.Module):
    """ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ìš© BERT ë¶„ë¥˜ê¸°"""
    def __init__(self, model_name, num_classes=2, dropout_rate=0.15, model_idx=0):
        super(LongSequenceBERTClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.model_name = model_name
        self.model_idx = model_idx
        
        # A100ì—ì„œëŠ” ë” ì ê²Œ í”„ë¦¬ì§• (ë©”ëª¨ë¦¬ ì¶©ë¶„)
        freeze_layers = [4, 5][model_idx % 2]  # ë” ë§ì€ ë ˆì´ì–´ í•™ìŠµ
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:freeze_layers]:
            for param in layer.parameters():
                param.requires_grad = False
        
        hidden_size = self.bert.config.hidden_size
        
        # ê¸´ ì‹œí€€ìŠ¤ìš© ê³ ê¸‰ ë¶„ë¥˜ê¸°
        self.pre_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # ëª¨ë¸ë³„ ë‹¤ë¥¸ ë¶„ë¥˜ê¸°
        if model_idx == 0:  # KoBERT
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.LayerNorm(hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_size // 2, hidden_size // 4),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_size // 4, num_classes)
            )
        else:  # KLUE-BERT
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.LayerNorm(hidden_size),
                nn.GELU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.GELU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_size // 2, num_classes)
            )
        
        # ì–´í…ì…˜ í’€ë§ (ê¸´ ì‹œí€€ìŠ¤ì—ì„œ íš¨ê³¼ì )
        self.attention_pooling = nn.MultiheadAttention(
            hidden_size, num_heads=16, batch_first=True, dropout=dropout_rate
        )
        
    def forward(self, input_ids, attention_mask):
        with torch.cuda.amp.autocast():
            # BERT ì¸ì½”ë”©
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš©
            sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden]
            
            # ì–´í…ì…˜ í’€ë§ìœ¼ë¡œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘
            pooled_output, attention_weights = self.attention_pooling(
                sequence_output, sequence_output, sequence_output,
                key_padding_mask=(attention_mask == 0)
            )
            
            # ê°€ì¤‘ í‰ê· 
            mask_expanded = attention_mask.unsqueeze(-1).float()
            weighted_output = (pooled_output * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)
            
            # ë¶„ë¥˜
            enhanced_output = self.pre_classifier(weighted_output)
            enhanced_output = enhanced_output + weighted_output  # ì”ì°¨ ì—°ê²°
            logits = self.classifier(enhanced_output)
        
        return logits, attention_weights

def analyze_text_lengths(texts):
    """í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„"""
    lengths = texts.str.len()
    print(f"\nğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
    print(f"  í‰ê· : {lengths.mean():.1f} ë¬¸ì")
    print(f"  ì¤‘ê°„ê°’: {lengths.median():.1f} ë¬¸ì")
    print(f"  95th percentile: {lengths.quantile(0.95):.1f} ë¬¸ì")
    print(f"  99th percentile: {lengths.quantile(0.99):.1f} ë¬¸ì")
    print(f"  ìµœëŒ€: {lengths.max()} ë¬¸ì")
    print(f"  1000ì ì´ìƒ: {(lengths > 1000).sum()}/{len(texts)} ({(lengths > 1000).mean()*100:.1f}%)")
    print(f"  2000ì ì´ìƒ: {(lengths > 2000).sum()}/{len(texts)} ({(lengths > 2000).mean()*100:.1f}%)")
    print(f"  3000ì ì´ìƒ: {(lengths > 3000).sum()}/{len(texts)} ({(lengths > 3000).mean()*100:.1f}%)")
    
    # MAX_LEN ì¶”ì²œ
    tokenizer_temp = AutoTokenizer.from_pretrained('klue/bert-base')
    sample_tokens = []
    for text in texts.sample(min(1000, len(texts))):
        tokens = tokenizer_temp.encode(text, add_special_tokens=True)
        sample_tokens.append(len(tokens))
    
    token_95th = np.percentile(sample_tokens, 95)
    token_99th = np.percentile(sample_tokens, 99)
    
    print(f"\nğŸ”¤ í† í° ê¸¸ì´ ë¶„ì„:")
    print(f"  95th percentile: {token_95th:.0f} í† í°")
    print(f"  99th percentile: {token_99th:.0f} í† í°")
    
    if token_95th <= 1024:
        print(f"  âœ… MAX_LEN=1024 ì¶”ì²œ (95% ì»¤ë²„)")
    elif token_95th <= 1536:
        print(f"  âš ï¸ MAX_LEN=1536 ê¶Œì¥ (95% ì»¤ë²„)")
    else:
        print(f"  ğŸš¨ MAX_LEN=2048 ê³ ë ¤ í•„ìš”")

def train_single_model(model, train_loader, val_loader, model_name, fold_idx=None):
    """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨ (ê¸´ ì‹œí€€ìŠ¤ ìµœì í™”)"""
    criterion = nn.CrossEntropyLoss()
    scaler = torch.cuda.amp.GradScaler()
    
    # A100 ìµœì í™” ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=LEARNING_RATE, 
        weight_decay=WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-6
    )
    
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    best_auc = 0
    best_model_state = None
    
    fold_str = f"Fold {fold_idx+1}" if fold_idx is not None else "Single"
    model_name_short = model_name.split('/')[-1]
    
    for epoch in range(EPOCHS):
        # í›ˆë ¨
        model.train()
        total_loss = 0
        correct_predictions = 0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'{fold_str} {model_name_short} Epoch {epoch+1}')):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            with torch.cuda.amp.autocast():
                logits, attention_weights = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(logits, labels)
            
            _, preds = torch.max(logits, dim=1)
            correct_predictions += torch.sum(preds == labels)
            total_loss += loss.item()
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            
            # ê¸´ ì‹œí€€ìŠ¤ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬
            if batch_idx % 20 == 0:
                torch.cuda.empty_cache()
        
        train_loss = total_loss / len(train_loader)
        train_acc = correct_predictions.double() / len(train_loader.dataset)
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        val_predictions = []
        val_probabilities = []
        val_labels = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc='Validation'):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                with torch.cuda.amp.autocast():
                    logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)
                    loss = criterion(logits, labels)
                
                val_loss += loss.item()
                probs = torch.softmax(logits, dim=1)
                val_probabilities.extend(probs[:, 1].cpu().numpy())
                val_labels.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_auc = roc_auc_score(val_labels, val_probabilities)
        val_f1 = f1_score(val_labels, [1 if p > 0.5 else 0 for p in val_probabilities])
        
        print(f'  Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
        print(f'             Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val F1: {val_f1:.4f}')
        
        if val_auc > best_auc:
            best_auc = val_auc
            best_model_state = model.state_dict().copy()
            print(f'  ğŸ¯ New best AUC: {best_auc:.4f}')
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            print(f'  GPU Memory: {memory_allocated:.2f}GB')
    
    model.load_state_dict(best_model_state)
    return model, best_auc

def predict_with_model(model, data_loader):
    """ë‹¨ì¼ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Predicting'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            with torch.cuda.amp.autocast():
                logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)
            
            probs = torch.softmax(logits, dim=1)
            predictions.extend(probs[:, 1].cpu().numpy())
    
    return np.array(predictions)

def ensemble_predict(models, tokenizers, test_data, weights=None):
    """ì•™ìƒë¸” ì˜ˆì¸¡"""
    if weights is None:
        weights = [1.0] * len(models)
    
    all_predictions = []
    
    for i, (model, tokenizer) in enumerate(zip(models, tokenizers)):
        print(f"\nì˜ˆì¸¡ ì¤‘: {ENSEMBLE_MODELS[i].split('/')[-1]}")
        
        test_dataset = LongSequenceDataset(test_data, labels=None, tokenizer=tokenizer, max_len=MAX_LEN)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
        
        predictions = predict_with_model(model, test_loader)
        all_predictions.append(predictions * weights[i])
    
    # ê°€ì¤‘ í‰ê· 
    ensemble_predictions = np.mean(all_predictions, axis=0)
    return ensemble_predictions

def main():
    print("=== ğŸš€ A100 Long Sequence Ensemble Training ===")
    print("Loading data...")
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    print("Preprocessing text data...")
    
    train['title'] = train['title'].fillna('').str.strip()
    train['full_text'] = train['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    train['combined_text'] = train['title'] + ' [SEP] ' + train['full_text']
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    analyze_text_lengths(train['combined_text'])
    
    X = train['combined_text']
    y = train['generated']
    
    print(f"\nClass distribution: {y.value_counts().to_dict()}")
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤ê³¼ í† í¬ë‚˜ì´ì €ë“¤
    ensemble_models = []
    ensemble_tokenizers = []
    model_weights = []
    
    print(f"\nğŸ¯ A100 Long Sequence ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘")
    print(f"ì„¤ì •: MAX_LEN={MAX_LEN}, BATCH_SIZE={BATCH_SIZE}, {N_FOLDS}-Fold")
    
    for model_idx, model_name in enumerate(ENSEMBLE_MODELS):
        print(f"\n{'='*60}")
        print(f"ëª¨ë¸ {model_idx+1}/{len(ENSEMBLE_MODELS)}: {model_name}")
        print(f"{'='*60}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
            print(f"Successfully loaded tokenizer: {model_name}")
        except:
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
                print(f"Loaded fast tokenizer: {model_name}")
            except Exception as e:
                print(f"Failed to load {model_name}: {e}")
                continue
        
        if USE_KFOLD:
            # K-Fold êµì°¨ ê²€ì¦
            fold_aucs = []
            
            skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
            
            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
                print(f"\n--- Fold {fold_idx+1}/{N_FOLDS} ---")
                
                X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
                
                # ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
                train_dataset = LongSequenceDataset(X_train_fold, y_train_fold, tokenizer, MAX_LEN)
                val_dataset = LongSequenceDataset(X_val_fold, y_val_fold, tokenizer, MAX_LEN)
                
                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
                
                # ê¸´ ì‹œí€€ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”
                model = LongSequenceBERTClassifier(model_name, num_classes=2, model_idx=model_idx)
                model = model.to(device)
                
                # í›ˆë ¨
                trained_model, fold_auc = train_single_model(
                    model, train_loader, val_loader, model_name, fold_idx
                )
                
                fold_aucs.append(fold_auc)
                
                # ì²« ë²ˆì§¸ í´ë“œ ëª¨ë¸ë§Œ ì €ì¥ (ì•™ìƒë¸”ìš©)
                if fold_idx == 0:
                    ensemble_models.append(trained_model)
                    ensemble_tokenizers.append(tokenizer)
                
                torch.cuda.empty_cache()
            
            avg_auc = np.mean(fold_aucs)
            model_weights.append(avg_auc)
            print(f"\n{model_name} K-Fold í‰ê·  AUC: {avg_auc:.4f}")
            
        else:
            # ë‹¨ì¼ í•™ìŠµ
            X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
            
            train_dataset = LongSequenceDataset(X_train, y_train, tokenizer, MAX_LEN)
            val_dataset = LongSequenceDataset(X_val, y_val, tokenizer, MAX_LEN)
            
            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
            
            model = LongSequenceBERTClassifier(model_name, num_classes=2, model_idx=model_idx)
            model = model.to(device)
            
            trained_model, auc = train_single_model(model, train_loader, val_loader, model_name)
            
            ensemble_models.append(trained_model)
            ensemble_tokenizers.append(tokenizer)
            model_weights.append(auc)
            
            print(f"\n{model_name} AUC: {auc:.4f}")
    
    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    total_weight = sum(model_weights)
    normalized_weights = [w / total_weight for w in model_weights]
    
    print(f"\nğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
    for i, (model_name, weight) in enumerate(zip(ENSEMBLE_MODELS, normalized_weights)):
        print(f"  {model_name.split('/')[-1]}: {weight:.3f}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬...")
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('').str.strip()
    test['full_text'] = test['full_text'].fillna('').str.replace(r'\n\s*\n', '\n', regex=True).str.strip()
    test['combined_text'] = test['title'] + ' [SEP] ' + test['full_text']
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸¸ì´ ë¶„ì„
    analyze_text_lengths(test['combined_text'])
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    print("\nğŸš€ A100 ê¸´ ì‹œí€€ìŠ¤ ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘...")
    ensemble_predictions = ensemble_predict(
        ensemble_models, ensemble_tokenizers, test['combined_text'], normalized_weights
    )
    
    print(f"\nğŸ“Š ì•™ìƒë¸” ì˜ˆì¸¡ í†µê³„:")
    print(f"  ì˜ˆì¸¡ ê°œìˆ˜: {len(ensemble_predictions)}")
    print(f"  í™•ë¥  ë²”ìœ„: {ensemble_predictions.min():.4f} - {ensemble_predictions.max():.4f}")
    print(f"  í‰ê·  í™•ë¥ : {ensemble_predictions.mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {ensemble_predictions.std():.4f}")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    sample_submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = ensemble_predictions
    sample_submission.to_csv('./long_sequence_submission.csv', index=False)
    
    print(f"\nğŸ‰ A100 ê¸´ ì‹œí€€ìŠ¤ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_LEN} (ì²­í‚¹ ì—†ìŒ)")
    print(f"  ì•™ìƒë¸” ëª¨ë¸ ìˆ˜: {len(ensemble_models)}")
    print(f"  ì‚¬ìš©ëœ ëª¨ë¸ë“¤:")
    for i, model_name in enumerate(ENSEMBLE_MODELS):
        print(f"    {i+1}. {model_name} (ê°€ì¤‘ì¹˜: {normalized_weights[i]:.3f})")
    print(f"  K-Fold: {N_FOLDS}-Fold")
    print(f"  ì œì¶œ íŒŒì¼: long_sequence_submission.csv")
    print(f"  ì˜ˆìƒ ì„±ëŠ¥: ê¸°ì¡´ ëŒ€ë¹„ +3~6% (ì •ë³´ ì†ì‹¤ ìµœì†Œí™”)")
    
    torch.cuda.empty_cache()
    print(sample_submission.head())

if __name__ == "__main__":
    main()
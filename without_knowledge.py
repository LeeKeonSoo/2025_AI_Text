import pandas as pd
import numpy as np
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.preprocessing import FunctionTransformer, StandardScaler
from sklearn.metrics import roc_auc_score

from transformers import AutoTokenizer, AutoModel
from xgboost import XGBClassifier
import lightgbm as lgb
from catboost import CatBoostClassifier

from scipy.sparse import hstack
from scipy.stats import entropy
from collections import Counter
import gc

print("ğŸš€ KoBERT + CPU ìµœì í™” ì „ëµ ì‹œì‘!")
print(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ’» GPU: {torch.cuda.get_device_name(0)}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ============ KoBERT ëª¨ë¸ ì •ì˜ ============
class KoBERTClassifier(nn.Module):
    def __init__(self, model_name='klue/bert-base', dropout=0.3):
        super(KoBERTClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return torch.sigmoid(logits)

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx] if self.labels is not None else 0
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.float)
        }

def train_kobert_model(train_texts, train_labels, val_texts, val_labels, epochs=3, batch_size=16):
    """KoBERT ëª¨ë¸ í›ˆë ¨"""
    print("ğŸ¤– KoBERT ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    
    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
    model = KoBERTClassifier('klue/bert-base').to(device)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤í•¨ìˆ˜
    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    criterion = nn.BCELoss()
    
    best_auc = 0
    best_model = None
    
    print(f"ğŸ‹ï¸ KoBERT í›ˆë ¨ ì‹œì‘ (ì—í¬í¬: {epochs}, ë°°ì¹˜: {batch_size})...")
    
    for epoch in range(epochs):
        # í›ˆë ¨
        model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs.squeeze(), labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (50 ë°°ì¹˜ë§ˆë‹¤)
            if batch_idx % 50 == 0:
                print(f"  ë°°ì¹˜ {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")
        
        # ê²€ì¦
        model.eval()
        val_predictions = []
        val_true = []
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)
                
                outputs = model(input_ids, attention_mask)
                
                val_predictions.extend(outputs.cpu().numpy().flatten())
                val_true.extend(labels.cpu().numpy())
        
        val_auc = roc_auc_score(val_true, val_predictions)
        avg_loss = total_loss / len(train_loader)
        
        print(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Val AUC: {val_auc:.4f}")
        
        if val_auc > best_auc:
            best_auc = val_auc
            best_model = model.state_dict().copy()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
    
    print(f"âœ… KoBERT í›ˆë ¨ ì™„ë£Œ! ìµœê³  AUC: {best_auc:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(best_model)
    return model, tokenizer, best_auc

def predict_kobert(model, tokenizer, texts, batch_size=16):
    """KoBERT ì˜ˆì¸¡"""
    model.eval()
    dataset = TextDataset(texts, None, tokenizer)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    predictions = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            outputs = model(input_ids, attention_mask)
            predictions.extend(outputs.cpu().numpy().flatten())
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if batch_idx % 50 == 0:
                print(f"  ì˜ˆì¸¡ ë°°ì¹˜ {batch_idx}/{len(loader)}")
    
    return np.array(predictions)

def extract_advanced_features(text):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ"""
    if pd.isna(text) or text == "":
        return np.zeros(15)
    
    text = str(text)
    words = text.split()
    
    if len(words) == 0:
        return np.zeros(15)
    
    features = []
    
    # ê¸°ë³¸ í†µê³„
    features.extend([
        len(text),  # ë¬¸ì ìˆ˜
        len(words),  # ë‹¨ì–´ ìˆ˜
        len(text) / len(words),  # í‰ê·  ë‹¨ì–´ ê¸¸ì´
        len(set(words)) / len(words),  # ì–´íœ˜ ë‹¤ì–‘ì„±
    ])
    
    # ë¬¸ì¥ ë¶„ì„
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_count = max(len(sentences), 1)
    
    features.extend([
        sentence_count,
        len(text) / sentence_count,  # í‰ê·  ë¬¸ì¥ ê¸¸ì´
    ])
    
    # êµ¬ë‘ì  ë¶„ì„
    features.extend([
        text.count('.') / len(text),
        text.count(',') / len(text),
        text.count('!') / len(text),
        text.count('?') / len(text),
    ])
    
    # í•œêµ­ì–´ íŠ¹ì„±
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    features.extend([
        korean_chars / len(text),  # í•œê¸€ ë¹„ìœ¨
        len(re.findall(r'[a-zA-Z]', text)) / len(text),  # ì˜ë¬¸ ë¹„ìœ¨
        len(re.findall(r'\\d', text)) / len(text),  # ìˆ«ì ë¹„ìœ¨
    ])
    
    # ê³ ê¸‰ íŒ¨í„´
    # ì—°ê²°ì–´ ì‚¬ìš©
    connectors = ['ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ']
    connector_count = sum(text.count(conn) for conn in connectors)
    features.append(connector_count / len(words))
    
    # ë°˜ë³µ íŒ¨í„´
    bigrams = [text[i:i+2] for i in range(len(text)-1)]
    if len(bigrams) > 0:
        bigram_entropy = entropy(list(Counter(bigrams).values()))
        features.append(bigram_entropy)
    else:
        features.append(0)
    
    return np.array(features)

def create_paragraph_context_features(test_df):
    """ë¬¸ë‹¨ ë§¥ë½ íŠ¹ì§• ìƒì„±"""
    features_list = []
    grouped = test_df.groupby('title')
    
    for title, group in grouped:
        group = group.sort_values('paragraph_index').reset_index(drop=True)
        
        for idx, row in group.iterrows():
            features = {}
            
            # ìœ„ì¹˜ ì •ë³´
            features['paragraph_index'] = row['paragraph_index']
            features['total_paragraphs'] = len(group)
            features['relative_position'] = row['paragraph_index'] / len(group)
            features['is_first'] = 1 if row['paragraph_index'] == 1 else 0
            features['is_last'] = 1 if row['paragraph_index'] == len(group) else 0
            
            # ê¸¸ì´ íŠ¹ì„±
            current_length = len(str(row['full_text']))
            all_lengths = [len(str(r['full_text'])) for _, r in group.iterrows()]
            features['length_vs_avg'] = current_length / (np.mean(all_lengths) + 1)
            features['length_vs_median'] = current_length / (np.median(all_lengths) + 1)
            
            features_list.append(features)
    
    return pd.DataFrame(features_list)

def main():
    # ============ ë°ì´í„° ë¡œë”© ============
    print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...")
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    train['title'] = train['title'].fillna('')
    train['full_text'] = train['full_text'].fillna('')
    
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('')
    test['full_text'] = test['full_text'].fillna('')
    
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {train.shape}")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test.shape}")
    print(f"âš–ï¸ í´ë˜ìŠ¤ ë¶„í¬: {train['generated'].value_counts().to_dict()}")
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„± (ì œëª© + ë³¸ë¬¸)
    train['combined_text'] = train['title'] + ' ' + train['full_text']
    test['combined_text'] = test['title'] + ' ' + test['full_text']
    
    # ============ ë°ì´í„° ë¶„í•  ============
    X = train[['title', 'full_text', 'combined_text']]
    y = train['generated']
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, stratify=y, test_size=0.2, random_state=42
    )
    
    # ============ 1. KoBERT íŠ¹ì§• ì¶”ì¶œ ============
    print("\\nğŸ¤– === KoBERT íŠ¹ì§• ì¶”ì¶œ ===")
    
    # KoBERT ëª¨ë¸ í›ˆë ¨
    kobert_model, tokenizer, kobert_auc = train_kobert_model(
        X_train['combined_text'].values,
        y_train.values,
        X_val['combined_text'].values,
        y_val.values,
        epochs=3,
        batch_size=16
    )
    
    # KoBERT íŠ¹ì§• ìƒì„± (í™•ë¥ ê°’)
    print("ğŸ”® KoBERT í›ˆë ¨ ë°ì´í„° íŠ¹ì§• ìƒì„± ì¤‘...")
    kobert_train_features = predict_kobert(kobert_model, tokenizer, X_train['combined_text'].values).reshape(-1, 1)
    
    print("ğŸ”® KoBERT ê²€ì¦ ë°ì´í„° íŠ¹ì§• ìƒì„± ì¤‘...")
    kobert_val_features = predict_kobert(kobert_model, tokenizer, X_val['combined_text'].values).reshape(-1, 1)
    
    print("ğŸ”® KoBERT í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§• ìƒì„± ì¤‘...")
    kobert_test_features = predict_kobert(kobert_model, tokenizer, test['combined_text'].values).reshape(-1, 1)
    
    print(f"âœ… KoBERT íŠ¹ì§• ìƒì„± ì™„ë£Œ!")
    
    # ============ 2. TF-IDF íŠ¹ì§• ============
    print("\\nğŸ”¤ === TF-IDF íŠ¹ì§• ì¶”ì¶œ ===")
    
    get_title = FunctionTransformer(lambda x: x['title'], validate=False)
    get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)
    
    tfidf_vectorizer = FeatureUnion([
        ('title', Pipeline([('selector', get_title),
                            ('tfidf', TfidfVectorizer(
                                ngram_range=(1,2), 
                                max_features=5000,
                                min_df=3,
                                max_df=0.95,
                                sublinear_tf=True
                            ))])),
        ('full_text', Pipeline([('selector', get_text), 
                                ('tfidf', TfidfVectorizer(
                                    ngram_range=(1,3), 
                                    max_features=15000,
                                    min_df=3,
                                    max_df=0.95,
                                    sublinear_tf=True
                                ))])),
    ])
    
    # TF-IDF ë³€í™˜
    print("ğŸ”„ TF-IDF ë²¡í„°í™” ì¤‘...")
    tfidf_train = tfidf_vectorizer.fit_transform(X_train[['title', 'full_text']])
    tfidf_val = tfidf_vectorizer.transform(X_val[['title', 'full_text']])
    tfidf_test = tfidf_vectorizer.transform(test[['title', 'full_text']])
    
    print(f"ğŸ“ TF-IDF íŠ¹ì§• ìˆ˜: {tfidf_train.shape[1]}")
    
    # ============ 3. ê³ ê¸‰ ìˆ˜ì‘ì—… íŠ¹ì§• ============
    print("\\nğŸ§  === ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ===")
    
    # ì œëª©ê³¼ ë³¸ë¬¸ì˜ ê³ ê¸‰ íŠ¹ì§•
    print("ğŸ” ì œëª© íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    train_title_features = np.array([extract_advanced_features(text) for text in X_train['title']])
    val_title_features = np.array([extract_advanced_features(text) for text in X_val['title']])
    test_title_features = np.array([extract_advanced_features(text) for text in test['title']])
    
    print("ğŸ” ë³¸ë¬¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    train_text_features = np.array([extract_advanced_features(text) for text in X_train['full_text']])
    val_text_features = np.array([extract_advanced_features(text) for text in X_val['full_text']])
    test_text_features = np.array([extract_advanced_features(text) for text in test['full_text']])
    
    # íŠ¹ì§• ê²°í•©
    advanced_train = np.hstack([train_title_features, train_text_features])
    advanced_val = np.hstack([val_title_features, val_text_features])
    advanced_test = np.hstack([test_title_features, test_text_features])
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    advanced_train = scaler.fit_transform(advanced_train)
    advanced_val = scaler.transform(advanced_val)
    advanced_test = scaler.transform(advanced_test)
    
    print(f"ğŸ§  ê³ ê¸‰ íŠ¹ì§• ìˆ˜: {advanced_train.shape[1]}")
    
    # ============ 4. ëª¨ë“  íŠ¹ì§• ê²°í•© ============
    print("\\nğŸ”— === íŠ¹ì§• í†µí•© ===")
    
    # ëª¨ë“  íŠ¹ì§• ê²°í•©
    X_train_combined = hstack([
        tfidf_train,
        kobert_train_features,
        advanced_train
    ])
    
    X_val_combined = hstack([
        tfidf_val,
        kobert_val_features,
        advanced_val
    ])
    
    X_test_combined = hstack([
        tfidf_test,
        kobert_test_features,
        advanced_test
    ])
    
    print(f"ğŸ¯ ìµœì¢… íŠ¹ì§• ìˆ˜: {X_train_combined.shape[1]:,}")
    
    # ============ 5. CPU ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨ ============
    print("\\nğŸš€ === CPU ìµœì í™” ëª¨ë¸ í›ˆë ¨ ===")
    
    models = {}
    val_predictions = {}
    
    # XGBoost (CPU ì „ìš©)
    print("ğŸŒ³ XGBoost (CPU) í›ˆë ¨ ì¤‘...")
    xgb_model = XGBClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='auc',
        tree_method='hist',  # CPU ìµœì í™”
        n_jobs=-1  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    )
    xgb_model.fit(X_train_combined, y_train)
    models['xgb'] = xgb_model
    val_predictions['xgb'] = xgb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… XGBoost AUC: {roc_auc_score(y_val, val_predictions['xgb']):.4f}")
    
    # LightGBM (CPU)
    print("âš¡ LightGBM (CPU) í›ˆë ¨ ì¤‘...")
    lgb_model = lgb.LGBMClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        objective='binary',
        metric='auc',
        device='cpu',
        n_jobs=-1,
        verbose=-1
    )
    lgb_model.fit(X_train_combined, y_train)
    models['lgb'] = lgb_model
    val_predictions['lgb'] = lgb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… LightGBM AUC: {roc_auc_score(y_val, val_predictions['lgb']):.4f}")
    
    # CatBoost (CPU)
    print("ğŸ± CatBoost (CPU) í›ˆë ¨ ì¤‘...")
    catb_model = CatBoostClassifier(
        iterations=500,
        depth=8,
        learning_rate=0.05,
        random_seed=42,
        task_type='CPU',
        thread_count=-1,
        verbose=False
    )
    catb_model.fit(X_train_combined, y_train)
    models['catb'] = catb_model
    val_predictions['catb'] = catb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… CatBoost AUC: {roc_auc_score(y_val, val_predictions['catb']):.4f}")
    
    # KoBERT ë‹¨ë… ì„±ëŠ¥
    val_predictions['kobert'] = kobert_val_features.flatten()
    print(f"ğŸ¤– KoBERT AUC: {kobert_auc:.4f}")
    
    # ============ 6. ìµœì  ì•™ìƒë¸” ============
    print("\\nğŸ¯ === ìµœì  ì•™ìƒë¸” ===")
    
    # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    aucs = {}
    for name, pred in val_predictions.items():
        auc = roc_auc_score(y_val, pred)
        aucs[name] = auc
    
    # ì§€ìˆ˜ì  ê°€ì¤‘ì¹˜
    total_weight = sum(np.exp(auc * 5) for auc in aucs.values())
    weights = {name: np.exp(auc * 5) / total_weight for name, auc in aucs.items()}
    
    print("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ ë° ê°€ì¤‘ì¹˜:")
    for name in weights:
        print(f"  {name}: AUC={aucs[name]:.4f}, Weight={weights[name]:.3f}")
    
    # ê°€ì¤‘ ì•™ìƒë¸”
    ensemble_pred = np.zeros(len(y_val))
    for name, pred in val_predictions.items():
        ensemble_pred += weights[name] * pred
    
    ensemble_auc = roc_auc_score(y_val, ensemble_pred)
    print(f"\\nğŸš€ ê°€ì¤‘ ì•™ìƒë¸” AUC: {ensemble_auc:.4f}")
    
    # ============ 7. ìµœì¢… ì˜ˆì¸¡ ============
    print("\\nğŸ”® === ìµœì¢… ì˜ˆì¸¡ ===")
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    print("ğŸ“Š ê° ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì¤‘...")
    test_predictions = {}
    test_predictions['xgb'] = models['xgb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['lgb'] = models['lgb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['catb'] = models['catb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['kobert'] = kobert_test_features.flatten()
    
    # ê°€ì¤‘ ì•™ìƒë¸” ì ìš©
    final_probs = np.zeros(len(test_predictions['xgb']))
    for name, pred in test_predictions.items():
        final_probs += weights[name] * pred
    
    # ============ 8. ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬ ============
    print("\\nğŸ¯ === ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬ ===")
    
    # ë¬¸ë‹¨ ë§¥ë½ íŠ¹ì§•
    test_context = create_paragraph_context_features(test)
    
    adjusted_probs = final_probs.copy()
    
    # titleë³„ ì¡°ì •
    for title in test['title'].unique():
        mask = test['title'] == title
        title_indices = test[mask].index
        title_probs = final_probs[mask]
        title_context = test_context[mask]
        
        if len(title_probs) > 1:
            avg_prob = np.mean(title_probs)
            smoothing_factor = 0.15
            
            for i, (idx, row) in enumerate(title_context.iterrows()):
                original_prob = title_probs[i]
                
                # ìœ„ì¹˜ ê¸°ë°˜ ì¡°ì •
                adjustment = 0
                if row['is_first'] == 1:
                    adjustment -= 0.03  # ì²« ë¬¸ë‹¨
                if row['relative_position'] > 0.8:
                    adjustment += 0.02  # ë§ˆì§€ë§‰ ë¶€ë¶„
                
                # ìŠ¤ë¬´ë”© + ì¡°ì •
                smoothed_prob = original_prob * (1 - smoothing_factor) + avg_prob * smoothing_factor
                final_prob = np.clip(smoothed_prob + adjustment, 0, 1)
                
                adjusted_probs[title_indices[i]] = final_prob
    
    print(f"ğŸ“ˆ ì¡°ì • ì •ë„: {np.mean(np.abs(adjusted_probs - final_probs)):.4f}")
    
    # ============ 9. ì œì¶œ íŒŒì¼ ìƒì„± ============
    print(f"\\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ë¶„í¬:")
    print(f"  ìµœì†Œê°’: {adjusted_probs.min():.4f}")
    print(f"  ìµœëŒ€ê°’: {adjusted_probs.max():.4f}")
    print(f"  í‰ê· ê°’: {adjusted_probs.mean():.4f}")
    
    sample_submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = adjusted_probs
    
    sample_submission.to_csv('./without_knowledge.csv', index=False)
    
    print(f"\\nğŸ‰ ìµœê³  ì„±ëŠ¥ ì œì¶œ íŒŒì¼ ìƒì„±: without_knowledge.csv")
    print(f"ğŸš€ ìµœì¢… ê²€ì¦ AUC: {ensemble_auc:.4f}")
    print(f"ğŸ† ì‚¬ìš©ëœ ê¸°ìˆ :")
    print(f"  - ğŸ¤– KoBERT (í•œêµ­ì–´ ì‚¬ì „í›ˆë ¨ ëª¨ë¸)")
    print(f"  - ğŸ”¤ ê³ ê¸‰ TF-IDF (1-3gram)")
    print(f"  - ğŸ§  ìˆ˜ì‘ì—… ì–¸ì–´í•™ì  íŠ¹ì§•")
    print(f"  - ğŸ¯ 4ê°œ ëª¨ë¸ ê°€ì¤‘ ì•™ìƒë¸”")
    print(f"  - ğŸ”§ ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬")
    print(f"  - ğŸ’» CPU ìµœì í™” í›ˆë ¨")
    print(f"ğŸ’ª CPU í™˜ê²½ì—ì„œë„ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!")

if __name__ == "__main__":
    main()
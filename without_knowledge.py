import pandas as pd
import numpy as np
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.preprocessing import FunctionTransformer, StandardScaler
from sklearn.metrics import roc_auc_score

from transformers import AutoTokenizer, AutoModel
from xgboost import XGBClassifier
import lightgbm as lgb
from catboost import CatBoostClassifier

from scipy.sparse import hstack
from scipy.stats import entropy
from collections import Counter
import gc
import os

# ë‹¨ì¼ ì‹¤í–‰ ì²´í¬ (ì¤‘ë³µ ë©”ì‹œì§€ ë°©ì§€)
if 'STRATEGY_INITIALIZED' not in os.environ:
    print("ğŸš€ VRAM 8GB + RAM 64GB ìµœëŒ€ í™œìš© ì „ëµ ì‹œì‘!")
    print(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ’» GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ¯ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        print("ğŸ’ª ë©”ëª¨ë¦¬ ì œí•œ í•´ì œ - ìµœëŒ€ í™œìš© ëª¨ë“œ!")
    os.environ['STRATEGY_INITIALIZED'] = '1'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš© ì„¤ì •
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.backends.cudnn.benchmark = True
    # ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€
    torch.cuda.set_per_process_memory_fraction(0.95)  # VRAM 95% ì‚¬ìš© í—ˆìš©

# ============ ê³ ì„±ëŠ¥ KoBERT ëª¨ë¸ (ë©”ëª¨ë¦¬ ì œí•œ í•´ì œ) ============
class HighPerformanceKoBERTClassifier(nn.Module):
    def __init__(self, model_name='klue/bert-base', dropout=0.3):
        super(HighPerformanceKoBERTClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        
        # ë” ê°•ë ¥í•œ í—¤ë“œ (RAM 64GB í™œìš©)
        self.dropout1 = nn.Dropout(dropout)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout2 = nn.Dropout(dropout * 0.5)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout3 = nn.Dropout(dropout * 0.5)
        self.classifier = nn.Linear(256, 1)
        self.relu = nn.ReLU()
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        # ë” í’ë¶€í•œ íŠ¹ì§• ì¶”ì¶œ (ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´ í‰ê· )
        hidden_states = outputs.last_hidden_state
        pooled_output = torch.mean(hidden_states, dim=1)  # Global average pooling
        
        # ë©€í‹° ë ˆì´ì–´ í—¤ë“œ
        x = self.dropout1(pooled_output)
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout2(x)
        
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout3(x)
        
        logits = self.classifier(x)
        return logits

class HighCapacityDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=384):  # ë” ê¸´ ì‹œí€€ìŠ¤
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])[:2000]  # ë” ê¸´ í…ìŠ¤íŠ¸ í—ˆìš©
        label = self.labels[idx] if self.labels is not None else 0
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.float32)
        }

def train_high_performance_kobert(train_texts, train_labels, val_texts, val_labels, epochs=4, batch_size=32):
    """ê³ ì„±ëŠ¥ KoBERT í›ˆë ¨ - ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©"""
    print("ğŸ¤– ê³ ì„±ëŠ¥ KoBERT ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    
    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
    model = HighPerformanceKoBERTClassifier('klue/bert-base').to(device)
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë¹„í™œì„±í™” (RAM 64GB í™œìš©)
    # model.bert.gradient_checkpointing_enable()  # ì£¼ì„ ì²˜ë¦¬
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = HighCapacityDataset(train_texts, train_labels, tokenizer)
    val_dataset = HighCapacityDataset(val_texts, val_labels, tokenizer)
    
    # ë” í° ë°°ì¹˜ì™€ ë” ë§ì€ ì›Œì»¤ (RAM 64GB í™œìš©)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=8, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, 
                           num_workers=8, pin_memory=True, persistent_workers=True)
    
    # ê³ ì„±ëŠ¥ ì˜µí‹°ë§ˆì´ì €
    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, eps=1e-8)
    criterion = nn.BCEWithLogitsLoss()
    
    # ë” ì ê·¹ì ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=len(train_loader), eta_min=1e-6)
    
    # GradScaler for mixed precision
    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
    
    best_auc = 0
    best_model = None
    
    print(f"ğŸ‹ï¸ ê³ ì„±ëŠ¥ KoBERT í›ˆë ¨ ì‹œì‘ (ì—í¬í¬: {epochs}, ë°°ì¹˜: {batch_size})...")
    
    for epoch in range(epochs):
        # í›ˆë ¨
        model.train()
        total_loss = 0
        batch_count = 0
        
        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = batch['label'].to(device, non_blocking=True)
            
            # í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
            if scaler is not None:
                with torch.cuda.amp.autocast():
                    logits = model(input_ids, attention_mask)
                    logits = logits.view(-1)
                    labels = labels.view(-1)
                    loss = criterion(logits, labels)
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(input_ids, attention_mask)
                logits = logits.view(-1)
                labels = labels.view(-1)
                loss = criterion(logits, labels)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            
            scheduler.step()
            total_loss += loss.item()
            batch_count += 1
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (ë§¤ 100 ë°°ì¹˜ë§ˆë‹¤ë¡œ ì¤„ì„)
            if batch_idx % 100 == 0:
                print(f"  ë°°ì¹˜ {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ê²€ì¦
        model.eval()
        val_predictions = []
        val_true = []
        
        print("ê²€ì¦ ì¤‘...")
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(device, non_blocking=True)
                labels = batch['label'].to(device, non_blocking=True)
                
                if torch.cuda.is_available():
                    with torch.cuda.amp.autocast():
                        logits = model(input_ids, attention_mask)
                else:
                    logits = model(input_ids, attention_mask)
                
                probs = torch.sigmoid(logits).view(-1)
                val_predictions.extend(probs.cpu().numpy())
                val_true.extend(labels.cpu().numpy())
        
        val_auc = roc_auc_score(val_true, val_predictions)
        avg_loss = total_loss / batch_count
        
        print(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Val AUC: {val_auc:.4f}")
        
        if val_auc > best_auc:
            best_auc = val_auc
            best_model = model.state_dict().copy()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëœ ìì£¼)
        if epoch % 2 == 0:  # 2 ì—í¬í¬ë§ˆë‹¤ë§Œ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
    
    print(f"âœ… ê³ ì„±ëŠ¥ KoBERT í›ˆë ¨ ì™„ë£Œ! ìµœê³  AUC: {best_auc:.4f}")
    
    model.load_state_dict(best_model)
    return model, tokenizer, best_auc

def predict_high_performance_kobert(model, tokenizer, texts, batch_size=48):
    """ê³ ì„±ëŠ¥ KoBERT ì˜ˆì¸¡"""
    model.eval()
    dataset = HighCapacityDataset(texts, None, tokenizer)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, 
                       num_workers=8, pin_memory=True, persistent_workers=True)
    
    predictions = []
    
    print("KoBERT ì˜ˆì¸¡ ì¤‘...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            
            if torch.cuda.is_available():
                with torch.cuda.amp.autocast():
                    logits = model(input_ids, attention_mask)
            else:
                logits = model(input_ids, attention_mask)
            
            probs = torch.sigmoid(logits).view(-1)
            predictions.extend(probs.cpu().numpy())
            
            # ì§„í–‰ìƒí™© (ë§¤ 200 ë°°ì¹˜ë§ˆë‹¤)
            if batch_idx % 200 == 0:
                print(f"  ì˜ˆì¸¡ ë°°ì¹˜ {batch_idx}/{len(loader)}")
    
    return np.array(predictions)

def extract_efficient_features(text):
    """íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ íŠ¹ì§• ì¶”ì¶œ"""
    if pd.isna(text) or text == "":
        return np.zeros(15)  # íŠ¹ì§• ìˆ˜ ì¤„ì„
    
    text = str(text)
    words = text.split()
    
    if len(words) == 0:
        return np.zeros(15)
    
    features = []
    
    # í•µì‹¬ í†µê³„ë§Œ (1-6)
    features.extend([
        len(text),  # ë¬¸ì ìˆ˜
        len(words),  # ë‹¨ì–´ ìˆ˜
        len(text) / len(words),  # í‰ê·  ë‹¨ì–´ ê¸¸ì´
        len(set(words)) / len(words),  # ì–´íœ˜ ë‹¤ì–‘ì„±
        np.mean([len(w) for w in words]),  # í‰ê·  ë‹¨ì–´ ê¸¸ì´
        np.std([len(w) for w in words]) if len(words) > 1 else 0,  # ë‹¨ì–´ ê¸¸ì´ í‘œì¤€í¸ì°¨
    ])
    
    # ì•ˆì „í•œ ë¬¸ì¥ íŠ¹ì§• (7-10)
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_count = max(len(sentences), 1)
    
    if sentence_count > 0:
        sentence_lengths = [len(s.split()) for s in sentences]
        if sentence_lengths:  # ì•ˆì „ì„± ì²´í¬
            features.extend([
                sentence_count,
                len(text) / sentence_count,  # í‰ê·  ë¬¸ì¥ ê¸¸ì´
                np.mean(sentence_lengths),  # í‰ê·  ë¬¸ì¥ ë‹¨ì–´ ìˆ˜
                np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0,
            ])
        else:
            features.extend([1, len(text), len(words), 0])
    else:
        features.extend([1, len(text), len(words), 0])
    
    # í•µì‹¬ êµ¬ë‘ì ë§Œ (11-13)
    features.extend([
        text.count('.') / len(text),
        text.count(',') / len(text),
        text.count('?') / len(text),
    ])
    
    # í•œêµ­ì–´ íŠ¹ì„± (14-15)
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    features.extend([
        korean_chars / len(text),  # í•œê¸€ ë¹„ìœ¨
        len(re.findall(r'[a-zA-Z]', text)) / len(text),  # ì˜ë¬¸ ë¹„ìœ¨
    ])
    
    return np.array(features)

def create_efficient_context_features(test_df):
    """íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ íŠ¹ì§• - ì—°ì‚°ëŸ‰ ìµœì í™”"""
    features_list = []
    grouped = test_df.groupby('title')
    
    print("íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ íŠ¹ì§• ìƒì„± ì¤‘...")
    for title, group in grouped:
        group = group.sort_values('paragraph_index').reset_index(drop=True)
        
        # ë¯¸ë¦¬ ê³„ì‚° (ë°˜ë³µ ì—°ì‚° ì¤„ì„)
        all_lengths = [len(str(r['full_text'])) for _, r in group.iterrows()]
        avg_length = np.mean(all_lengths)
        median_length = np.median(all_lengths)
        
        for idx, row in group.iterrows():
            features = {}
            
            # í•µì‹¬ ìœ„ì¹˜ ì •ë³´ë§Œ
            features['paragraph_index'] = row['paragraph_index']
            features['total_paragraphs'] = len(group)
            features['relative_position'] = row['paragraph_index'] / len(group)
            features['is_first'] = 1 if row['paragraph_index'] == 1 else 0
            features['is_last'] = 1 if row['paragraph_index'] == len(group) else 0
            
            # í•µì‹¬ ê¸¸ì´ íŠ¹ì„±ë§Œ
            current_length = len(str(row['full_text']))
            features.update({
                'current_length': current_length,
                'length_vs_avg': current_length / (avg_length + 1),
                'length_vs_median': current_length / (median_length + 1),
            })
            
            # ê°„ë‹¨í•œ ì¸ì ‘ ê´€ê³„ë§Œ
            if idx > 0:
                prev_length = len(str(group.iloc[idx-1]['full_text']))
                features['prev_length_ratio'] = current_length / (prev_length + 1)
            else:
                features['prev_length_ratio'] = 1.0
                
            if idx < len(group) - 1:
                next_length = len(str(group.iloc[idx+1]['full_text']))
                features['next_length_ratio'] = current_length / (next_length + 1)
            else:
                features['next_length_ratio'] = 1.0
            
            features_list.append(features)
    
    return pd.DataFrame(features_list)

def main():
    # ============ ë°ì´í„° ë¡œë”© ============
    print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...")
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    train['title'] = train['title'].fillna('')
    train['full_text'] = train['full_text'].fillna('')
    
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('')
    test['full_text'] = test['full_text'].fillna('')
    
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {train.shape}")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test.shape}")
    print(f"âš–ï¸ í´ë˜ìŠ¤ ë¶„í¬: {train['generated'].value_counts().to_dict()}")
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„± (ì œëª© + ë³¸ë¬¸)
    train['combined_text'] = train['title'] + ' ' + train['full_text']
    test['combined_text'] = test['title'] + ' ' + test['full_text']
    
    # ============ ë°ì´í„° ë¶„í•  ============
    X = train[['title', 'full_text', 'combined_text']]
    y = train['generated']
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, stratify=y, test_size=0.2, random_state=42
    )
    
    # ============ ê³ ì„±ëŠ¥ KoBERT íŠ¹ì§• ì¶”ì¶œ ============
    print("\\nğŸ¤– === ê³ ì„±ëŠ¥ KoBERT íŠ¹ì§• ì¶”ì¶œ ===")
    
    # KoBERT ëª¨ë¸ í›ˆë ¨ (ë” í° ë°°ì¹˜, ë” ê¸´ ì‹œí€€ìŠ¤)
    kobert_model, tokenizer, kobert_auc = train_high_performance_kobert(
        X_train['combined_text'].values,
        y_train.values,
        X_val['combined_text'].values,
        y_val.values,
        epochs=4,  # ë” ë§ì€ ì—í¬í¬
        batch_size=32  # ë” í° ë°°ì¹˜
    )
    
    # KoBERT íŠ¹ì§• ìƒì„±
    print("ğŸ”® KoBERT íŠ¹ì§• ìƒì„± ì¤‘...")
    kobert_train_features = predict_high_performance_kobert(kobert_model, tokenizer, X_train['combined_text'].values, batch_size=48).reshape(-1, 1)
    kobert_val_features = predict_high_performance_kobert(kobert_model, tokenizer, X_val['combined_text'].values, batch_size=48).reshape(-1, 1)
    kobert_test_features = predict_high_performance_kobert(kobert_model, tokenizer, test['combined_text'].values, batch_size=48).reshape(-1, 1)
    
    print(f"âœ… KoBERT íŠ¹ì§• ìƒì„± ì™„ë£Œ! AUC: {kobert_auc:.4f}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del kobert_model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # ============ ê³ ìš©ëŸ‰ TF-IDF íŠ¹ì§• ============
    print("\\nğŸ”¤ === ê³ ìš©ëŸ‰ TF-IDF íŠ¹ì§• ì¶”ì¶œ ===")
    
    get_title = FunctionTransformer(lambda x: x['title'], validate=False)
    get_text = FunctionTransformer(lambda x: x['full_text'], validate=False)
    
    # ë” ë§ì€ íŠ¹ì§• (RAM 64GB í™œìš©)
    tfidf_vectorizer = FeatureUnion([
        ('title', Pipeline([('selector', get_title),
                            ('tfidf', TfidfVectorizer(
                                ngram_range=(1,3),  # ë” ê¸´ n-gram
                                max_features=8000,  # ë” ë§ì€ íŠ¹ì§•
                                min_df=2,
                                max_df=0.95,
                                sublinear_tf=True
                            ))])),
        ('full_text', Pipeline([('selector', get_text), 
                                ('tfidf', TfidfVectorizer(
                                    ngram_range=(1,3),  # ë” ê¸´ n-gram
                                    max_features=20000,  # ë” ë§ì€ íŠ¹ì§•
                                    min_df=2,
                                    max_df=0.95,
                                    sublinear_tf=True
                                ))])),
    ])
    
    # TF-IDF ë³€í™˜
    print("ğŸ”„ ê³ ìš©ëŸ‰ TF-IDF ë²¡í„°í™” ì¤‘...")
    tfidf_train = tfidf_vectorizer.fit_transform(X_train[['title', 'full_text']])
    tfidf_val = tfidf_vectorizer.transform(X_val[['title', 'full_text']])
    tfidf_test = tfidf_vectorizer.transform(test[['title', 'full_text']])
    
    print(f"ğŸ“ TF-IDF íŠ¹ì§• ìˆ˜: {tfidf_train.shape[1]:,}")
    
    # ============ íš¨ìœ¨ì  ìˆ˜ì‘ì—… íŠ¹ì§• ============
    print("\\nğŸ§  === íš¨ìœ¨ì  íŠ¹ì§• ì¶”ì¶œ ===")
    
    # ì œëª©ê³¼ ë³¸ë¬¸ íŠ¹ì§• (ì—°ì‚°ëŸ‰ ìµœì í™”)
    print("ğŸ” ì œëª© íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    train_title_features = np.array([extract_efficient_features(text) for text in X_train['title']])
    val_title_features = np.array([extract_efficient_features(text) for text in X_val['title']])
    test_title_features = np.array([extract_efficient_features(text) for text in test['title']])
    
    print("ğŸ” ë³¸ë¬¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    train_text_features = np.array([extract_efficient_features(text) for text in X_train['full_text']])
    val_text_features = np.array([extract_efficient_features(text) for text in X_val['full_text']])
    test_text_features = np.array([extract_efficient_features(text) for text in test['full_text']])
    
    # íŠ¹ì§• ê²°í•© ë° ìŠ¤ì¼€ì¼ë§
    efficient_train = np.hstack([train_title_features, train_text_features])
    efficient_val = np.hstack([val_title_features, val_text_features])
    efficient_test = np.hstack([test_title_features, test_text_features])
    
    scaler = StandardScaler()
    efficient_train = scaler.fit_transform(efficient_train)
    efficient_val = scaler.transform(efficient_val)
    efficient_test = scaler.transform(efficient_test)
    
    print(f"ğŸ§  íš¨ìœ¨ì  íŠ¹ì§• ìˆ˜: {efficient_train.shape[1]}")
    
    # ============ ëª¨ë“  íŠ¹ì§• ê²°í•© ============
    print("\\nğŸ”— === íŠ¹ì§• í†µí•© ===")
    
    X_train_combined = hstack([
        tfidf_train,
        kobert_train_features,
        efficient_train
    ])
    
    X_val_combined = hstack([
        tfidf_val,
        kobert_val_features,
        efficient_val
    ])
    
    X_test_combined = hstack([
        tfidf_test,
        kobert_test_features,
        efficient_test
    ])
    
    print(f"ğŸ¯ ìµœì¢… íŠ¹ì§• ìˆ˜: {X_train_combined.shape[1]:,}")
    
    # ============ ê³ ì„±ëŠ¥ ëª¨ë¸ ì•™ìƒë¸” ============
    print("\\nğŸš€ === ê³ ì„±ëŠ¥ ëª¨ë¸ ì•™ìƒë¸” ===")
    
    models = {}
    val_predictions = {}
    
    # XGBoost (ê³ ì„±ëŠ¥ ì„¤ì •)
    print("ğŸŒ³ ê³ ì„±ëŠ¥ XGBoost í›ˆë ¨ ì¤‘...")
    xgb_model = XGBClassifier(
        n_estimators=500,  # ë” ë§ì€ íŠ¸ë¦¬
        max_depth=8,
        learning_rate=0.03,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='auc',
        tree_method='hist',
        n_jobs=-1
    )
    xgb_model.fit(X_train_combined, y_train)
    models['xgb'] = xgb_model
    val_predictions['xgb'] = xgb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… XGBoost AUC: {roc_auc_score(y_val, val_predictions['xgb']):.4f}")
    
    # LightGBM (ê³ ì„±ëŠ¥ ì„¤ì •)
    print("âš¡ ê³ ì„±ëŠ¥ LightGBM í›ˆë ¨ ì¤‘...")
    lgb_model = lgb.LGBMClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.03,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        objective='binary',
        metric='auc',
        device='cpu',
        n_jobs=-1,
        verbose=-1
    )
    lgb_model.fit(X_train_combined, y_train)
    models['lgb'] = lgb_model
    val_predictions['lgb'] = lgb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… LightGBM AUC: {roc_auc_score(y_val, val_predictions['lgb']):.4f}")
    
    # CatBoost (ê³ ì„±ëŠ¥ ì„¤ì •)
    print("ğŸ± ê³ ì„±ëŠ¥ CatBoost í›ˆë ¨ ì¤‘...")
    catb_model = CatBoostClassifier(
        iterations=500,
        depth=8,
        learning_rate=0.03,
        random_seed=42,
        task_type='CPU',
        thread_count=-1,
        verbose=False
    )
    catb_model.fit(X_train_combined, y_train)
    models['catb'] = catb_model
    val_predictions['catb'] = catb_model.predict_proba(X_val_combined)[:, 1]
    print(f"âœ… CatBoost AUC: {roc_auc_score(y_val, val_predictions['catb']):.4f}")
    
    # KoBERT ë‹¨ë… ì„±ëŠ¥
    val_predictions['kobert'] = kobert_val_features.flatten()
    kobert_val_auc = roc_auc_score(y_val, val_predictions['kobert'])
    print(f"ğŸ¤– KoBERT AUC: {kobert_val_auc:.4f}")
    
    # ============ ê³ ê¸‰ ì•™ìƒë¸” ============
    print("\\nğŸ¯ === ê³ ê¸‰ ì•™ìƒë¸” ===")
    
    # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    aucs = {}
    for name, pred in val_predictions.items():
        auc = roc_auc_score(y_val, pred)
        aucs[name] = auc
    
    # ë” ì ê·¹ì ì¸ ê°€ì¤‘ì¹˜ (ì„±ëŠ¥ ì°¨ì´ ê·¹ëŒ€í™”)
    total_weight = sum(np.exp(auc * 8) for auc in aucs.values())  # ë” í° ì§€ìˆ˜
    weights = {name: np.exp(auc * 8) / total_weight for name, auc in aucs.items()}
    
    print("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ ë° ê°€ì¤‘ì¹˜:")
    for name in sorted(weights.keys(), key=lambda x: aucs[x], reverse=True):
        print(f"  {name}: AUC={aucs[name]:.4f}, Weight={weights[name]:.3f}")
    
    # ê°€ì¤‘ ì•™ìƒë¸”
    ensemble_pred = np.zeros(len(y_val))
    for name, pred in val_predictions.items():
        ensemble_pred += weights[name] * pred
    
    ensemble_auc = roc_auc_score(y_val, ensemble_pred)
    print(f"\\nğŸš€ ê³ ê¸‰ ì•™ìƒë¸” AUC: {ensemble_auc:.4f}")
    
    # ============ ìµœì¢… ì˜ˆì¸¡ ============
    print("\\nğŸ”® === ìµœì¢… ì˜ˆì¸¡ ===")
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    print("ğŸ“Š ê° ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì¤‘...")
    test_predictions = {}
    test_predictions['xgb'] = models['xgb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['lgb'] = models['lgb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['catb'] = models['catb'].predict_proba(X_test_combined)[:, 1]
    test_predictions['kobert'] = kobert_test_features.flatten()
    
    # ê°€ì¤‘ ì•™ìƒë¸” ì ìš©
    final_probs = np.zeros(len(test_predictions['xgb']))
    for name, pred in test_predictions.items():
        final_probs += weights[name] * pred
    
    # ============ íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬ ============
    print("\\nğŸ¯ === íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬ ===")
    
    # íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ íŠ¹ì§•
    test_context = create_efficient_context_features(test)
    
    adjusted_probs = final_probs.copy()
    
    # ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ titleë³„ ì¡°ì •
    adjustment_count = 0
    for title in test['title'].unique():
        mask = test['title'] == title
        title_indices = test[mask].index
        title_probs = final_probs[mask]
        title_context = test_context[mask]
        
        if len(title_probs) > 1:
            # ê°„ë‹¨í•œ ìŠ¤ë¬´ë”©
            avg_prob = np.mean(title_probs)
            smoothing_factor = 0.15
            
            for i, (idx, row) in enumerate(title_context.iterrows()):
                original_prob = title_probs[i]
                
                # í•µì‹¬ ì¡°ì •ë§Œ
                adjustment = 0
                if row['is_first'] == 1:
                    adjustment -= 0.03  # ì²« ë¬¸ë‹¨
                if row['relative_position'] > 0.8:
                    adjustment += 0.02  # ë§ˆì§€ë§‰ ë¶€ë¶„
                
                # ê¸¸ì´ ê¸°ë°˜ ê°„ë‹¨ ì¡°ì •
                if row['length_vs_avg'] > 2.0:
                    adjustment += 0.02  # ë„ˆë¬´ ê¸´ ë¬¸ë‹¨
                elif row['length_vs_avg'] < 0.3:
                    adjustment += 0.025  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨
                
                # ìŠ¤ë¬´ë”© + ì¡°ì •
                smoothed_prob = original_prob * (1 - smoothing_factor) + avg_prob * smoothing_factor
                final_prob = np.clip(smoothed_prob + adjustment, 0.01, 0.99)
                
                adjusted_probs[title_indices[i]] = final_prob
                
                if abs(final_prob - original_prob) > 0.001:
                    adjustment_count += 1
    
    avg_adjustment = np.mean(np.abs(adjusted_probs - final_probs))
    print(f"ğŸ“ˆ í‰ê·  ì¡°ì • ì •ë„: {avg_adjustment:.4f}")
    print(f"ğŸ“Š ì¡°ì •ëœ ë¬¸ë‹¨ ìˆ˜: {adjustment_count}/{len(test)} ({adjustment_count/len(test)*100:.1f}%)")
    
    # ============ í’ˆì§ˆ ê²€ì¦ ë° ì œì¶œ ============
    print(f"\\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ë¶„í¬:")
    print(f"  ìµœì†Œê°’: {adjusted_probs.min():.4f}")
    print(f"  ìµœëŒ€ê°’: {adjusted_probs.max():.4f}")
    print(f"  í‰ê· ê°’: {adjusted_probs.mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {adjusted_probs.std():.4f}")
    print(f"  25% ë¶„ìœ„: {np.percentile(adjusted_probs, 25):.4f}")
    print(f"  75% ë¶„ìœ„: {np.percentile(adjusted_probs, 75):.4f}")
    
    sample_submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = adjusted_probs
    
    sample_submission.to_csv('./without_knowledge.csv', index=False)
    
    print(f"\\nğŸ‰ ìµœê³  ì„±ëŠ¥ ì œì¶œ íŒŒì¼ ìƒì„±: without_knowledge.csv")
    print(f"ğŸš€ ìµœì¢… ê²€ì¦ AUC: {ensemble_auc:.4f}")
    print(f"ğŸ’» íš¨ìœ¨ì ì´ë©´ì„œ ê³ ì„±ëŠ¥ ë‹¬ì„±:")
    print(f"  - ğŸ¤– ê³ ì„±ëŠ¥ KoBERT (ë°°ì¹˜32, ê¸¸ì´384, ë©€í‹°ë ˆì´ì–´í—¤ë“œ)")
    print(f"  - ğŸ”¤ ê³ ìš©ëŸ‰ TF-IDF ({tfidf_train.shape[1]:,} íŠ¹ì§•)")
    print(f"  - ğŸ§  íš¨ìœ¨ì  íŠ¹ì§• ({efficient_train.shape[1]} íŠ¹ì§•)")
    print(f"  - ğŸ¯ 4ê°œ ëª¨ë¸ ê³ ì„±ëŠ¥ ì•™ìƒë¸” (500 íŠ¸ë¦¬)")
    print(f"  - ğŸ”§ íš¨ìœ¨ì  ë¬¸ë‹¨ ë§¥ë½ í›„ì²˜ë¦¬")
    print(f"  - âš¡ ì—°ì‚°ëŸ‰ ìµœì í™”ë¡œ ë¹ ë¥¸ ì‹¤í–‰")
    print(f"ğŸ’ª íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ì™„ë²½í•œ ê· í˜•!")

if __name__ == "__main__":
    main()
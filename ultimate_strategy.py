#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ê·¹ëŒ€í™” ì „ëµ: ìµœê³  ì„±ëŠ¥ AI vs Human í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°
- ìŠ¤íƒ€ì¼ë¡œë©”íŠ¸ë¦¬ íŠ¹ì§• (ë¬¸ì²´ ë¶„ì„)
- ë‹¤ì¸µ TF-IDF (ë‹¨ì–´ + ë¬¸ì ë ˆë²¨)
- í† í”½ ëª¨ë¸ë§ (LDA)
- 5ê°œ ëª¨ë¸ ê°€ì¤‘ ì•™ìƒë¸”
"""

import pandas as pd
import numpy as np
import re
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import LatentDirichletAllocation

import lightgbm as lgb
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

from scipy.sparse import hstack
from scipy.stats import entropy
from collections import Counter

print("ğŸ¯ ê·¹ëŒ€í™” ì „ëµ ì‹œì‘!")

def extract_stylometric_features(text):
    """ìŠ¤íƒ€ì¼ë¡œë©”íŠ¸ë¦¬ íŠ¹ì§• ì¶”ì¶œ - AI vs Human ë¬¸ì²´ ì°¨ì´ í¬ì°©"""
    if pd.isna(text) or text == "":
        return np.zeros(25)
    
    text = str(text)
    char_count = len(text)
    words = text.split()
    word_count = len(words)
    
    if word_count == 0:
        return np.zeros(25)
    
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_count = max(len(sentences), 1)
    
    features = []
    
    # ê¸°ë³¸ ê¸¸ì´ í†µê³„ (AIëŠ” ë³´í†µ ì¼ì •í•œ ê¸¸ì´ íŒ¨í„´)
    features.extend([
        char_count,
        word_count,
        sentence_count,
        char_count / word_count
    ])
    
    # ì–´íœ˜ ë‹¤ì–‘ì„± (AIëŠ” ë°˜ë³µì  ì–´íœ˜ ì‚¬ìš© ê²½í–¥)
    unique_words = len(set(words))
    word_freq = Counter(words)
    hapax_legomena = sum(1 for count in word_freq.values() if count == 1)
    
    features.extend([
        unique_words / word_count,  # Type-Token Ratio
        hapax_legomena / word_count,  # í•œ ë²ˆë§Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ë¹„ìœ¨
        np.mean([len(word) for word in words]),
        np.std([len(word) for word in words]) if len(words) > 1 else 0
    ])
    
    # êµ¬ë‘ì  íŒ¨í„´ (AIëŠ” íŠ¹ì • êµ¬ë‘ì  ì„ í˜¸)
    for mark in ['.', ',', '!', '?']:
        features.append(text.count(mark) / char_count)
    
    # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ (AIëŠ” ê· ì¼í•œ ë¬¸ì¥ ê¸¸ì´)
    sentence_lengths = [len(s.split()) for s in sentences]
    features.extend([
        np.mean(sentence_lengths),
        np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0,
        np.median(sentence_lengths),
        max(sentence_lengths) - min(sentence_lengths)
    ])
    
    # ì–¸ì–´ë³„ ë¬¸ì ë¶„í¬
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    digit_chars = len(re.findall(r'\\d', text))
    
    features.extend([
        korean_chars / char_count,
        english_chars / char_count,
        digit_chars / char_count,
        (korean_chars + english_chars) / char_count
    ])
    
    # ê³ ê¸‰ íŒ¨í„´ ë¶„ì„
    bigrams = [text[i:i+2] for i in range(len(text)-1)]
    bigram_entropy = entropy(list(Counter(bigrams).values())) if len(bigrams) > 0 else 0
    
    features.extend([
        bigram_entropy,
        text.count('(') + text.count('['),  # ê´„í˜¸ ì‚¬ìš©
        text.count('"') + text.count("'"),  # ì¸ìš©ë¶€í˜¸
        len(re.findall(r'\\b[A-Z][a-z]+', text)),  # ëŒ€ë¬¸ì ë‹¨ì–´
        len(re.findall(r'\\d+', text))  # ìˆ«ì íŒ¨í„´
    ])
    
    return np.array(features)

def extract_semantic_features(text):
    """ì˜ë¯¸ë¡ ì  íŠ¹ì§• - AIì˜ ì˜ë¯¸ì  ì¼ê´€ì„±ê³¼ ì¸ê°„ì˜ ì°½ì˜ì„± êµ¬ë¶„"""
    if pd.isna(text) or text == "":
        return np.zeros(10)
    
    text = str(text)
    words = text.split()
    
    if len(words) == 0:
        return np.zeros(10)
    
    features = []
    
    # ì–´íœ˜ ë³µì¡ë„ (AIëŠ” ë³µì¡í•œ ë‹¨ì–´ ì„ í˜¸)
    long_words = [w for w in words if len(w) > 6]
    features.append(len(long_words) / len(words))
    
    # ë¹„ì •ìƒì  íŒ¨í„´
    unusual_patterns = len(re.findall(r'[ã„±-ã…ã…-ã…£]', text))
    features.append(unusual_patterns / len(text))
    
    # ë‹¨ì–´ ë°˜ë³µì„±
    word_repetition = len(words) - len(set(words))
    features.append(word_repetition / len(words))
    
    # ë¬¸ì²´ ì¼ê´€ì„±
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) > 1:
        sentence_starts = [s.split()[0] if s.split() else '' for s in sentences]
        start_diversity = len(set(sentence_starts)) / len(sentences)
        features.append(start_diversity)
        
        sentence_endings = [s[-1] if s else '' for s in sentences]
        ending_diversity = len(set(sentence_endings)) / len(sentences)
        features.append(ending_diversity)
    else:
        features.extend([0, 0])
    
    # ì—°ê²°ì–´ ì‚¬ìš© (AIëŠ” ë…¼ë¦¬ì  ì—°ê²°ì–´ ì„ í˜¸)
    connectors = ['ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ì¦‰', 'ì˜ˆë¥¼ ë“¤ì–´']
    connector_count = sum(text.count(conn) for conn in connectors)
    features.append(connector_count / len(words))
    
    # ê°ì • í‘œí˜„
    features.extend([
        text.count('?') / len(sentences),  # ì˜ë¬¸ë¬¸
        text.count('!') / len(sentences),  # ê°íƒ„ë¬¸
        len(re.findall(r'[ê°€-í£]', text)) / len(words),  # í•œê¸€ ìŒì ˆ ë°€ë„
    ])
    
    # ë¬¸ë‹¨ ì‘ì§‘ì„± (AIëŠ” ì¼ê´€ëœ ê¸¸ì´)
    if len(sentences) > 1:
        sent_lengths = [len(s.split()) for s in sentences]
        avg_len = np.mean(sent_lengths)
        similar_length_ratio = sum(1 for l in sent_lengths if abs(l - avg_len) < avg_len * 0.3) / len(sentences)
        features.append(similar_length_ratio)
    else:
        features.append(0)
    
    return np.array(features)

def create_advanced_features(df):
    """ëª¨ë“  ê³ ê¸‰ íŠ¹ì§• ìƒì„±"""
    print("ğŸ” ìŠ¤íƒ€ì¼ë¡œë©”íŠ¸ë¦¬ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    
    # ì œëª©ê³¼ ë³¸ë¬¸ì˜ ê°ì¢… íŠ¹ì§•ë“¤
    title_style = np.array([extract_stylometric_features(text) for text in df['title']])
    title_semantic = np.array([extract_semantic_features(text) for text in df['title']])
    text_style = np.array([extract_stylometric_features(text) for text in df['full_text']])
    text_semantic = np.array([extract_semantic_features(text) for text in df['full_text']])
    
    # ì œëª©-ë³¸ë¬¸ ê´€ê³„ íŠ¹ì§•
    relationships = []
    for i in range(len(df)):
        title_len = len(str(df.iloc[i]['title']))
        text_len = len(str(df.iloc[i]['full_text']))
        title_words = len(str(df.iloc[i]['title']).split())
        text_words = len(str(df.iloc[i]['full_text']).split())
        
        relationships.append([
            title_len / (text_len + 1),
            title_words / (text_words + 1),
            min(title_len, text_len) / (max(title_len, text_len) + 1)
        ])
    
    relationships = np.array(relationships)
    
    # ëª¨ë“  íŠ¹ì§• ê²°í•©
    all_features = np.hstack([
        title_style, title_semantic,
        text_style, text_semantic,
        relationships
    ])
    
    print(f"ğŸ’ ê³ ê¸‰ íŠ¹ì§• ìˆ˜: {all_features.shape[1]}")
    return all_features

def main():
    # ë°ì´í„° ë¡œë”©
    print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...")
    train = pd.read_csv('./train.csv', encoding='utf-8-sig')
    test = pd.read_csv('./test.csv', encoding='utf-8-sig')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    train['title'] = train['title'].fillna('')
    train['full_text'] = train['full_text'].fillna('')
    
    test = test.rename(columns={'paragraph_text': 'full_text'})
    test['title'] = test['title'].fillna('')
    test['full_text'] = test['full_text'].fillna('')
    
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {train.shape}")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test.shape}")
    print(f"âš–ï¸ í´ë˜ìŠ¤ ë¶„í¬: {train['generated'].value_counts().to_dict()}")
    
    # 1. ê³ ê¸‰ ìˆ˜ì‘ì—… íŠ¹ì§•
    print("\\nğŸ§  === ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ===")
    train_advanced = create_advanced_features(train)
    test_advanced = create_advanced_features(test)
    
    # 2. ë‹¤ì–‘í•œ TF-IDF ë²¡í„°í™”
    print("\\nğŸ”¤ === ë‹¤ì¸µ TF-IDF ë²¡í„°í™” ===")
    
    # Word-level TF-IDF (ì˜ë¯¸ ë‹¨ìœ„)
    tfidf_word = TfidfVectorizer(
        ngram_range=(1,3),  # 1-3gramìœ¼ë¡œ ë¬¸ë§¥ í¬ì°©
        max_features=15000,
        min_df=3,
        max_df=0.9,
        sublinear_tf=True,
        analyzer='word'
    )
    
    # Character-level TF-IDF (ìŠ¤íƒ€ì¼ íŒ¨í„´)
    tfidf_char = TfidfVectorizer(
        ngram_range=(3,5),  # ë¬¸ì íŒ¨í„´ìœ¼ë¡œ ìŠ¤íƒ€ì¼ í¬ì°©
        max_features=10000,
        min_df=5,
        analyzer='char',
        sublinear_tf=True
    )
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
    train_full = train['title'] + ' ' + train['full_text']
    test_full = test['title'] + ' ' + test['full_text']
    
    # TF-IDF ë³€í™˜
    train_word_tfidf = tfidf_word.fit_transform(train_full)
    test_word_tfidf = tfidf_word.transform(test_full)
    
    train_char_tfidf = tfidf_char.fit_transform(train_full)
    test_char_tfidf = tfidf_char.transform(test_full)
    
    print(f"ğŸ“ Word TF-IDF íŠ¹ì§•: {train_word_tfidf.shape[1]}")
    print(f"ğŸ”  Char TF-IDF íŠ¹ì§•: {train_char_tfidf.shape[1]}")
    
    # 3. í† í”½ ëª¨ë¸ë§ (ì£¼ì œ ë¶„í¬)
    print("\\nğŸ­ === í† í”½ ëª¨ë¸ë§ (LDA) ===")
    lda = LatentDirichletAllocation(
        n_components=50,  # 50ê°œ ì£¼ì œ
        random_state=42,
        max_iter=10
    )
    
    count_vect = CountVectorizer(
        max_features=5000,
        min_df=5,
        max_df=0.8,
        ngram_range=(1,2)
    )
    
    train_counts = count_vect.fit_transform(train_full)
    test_counts = count_vect.transform(test_full)
    
    train_topics = lda.fit_transform(train_counts)
    test_topics = lda.transform(test_counts)
    
    print(f"ğŸ¯ í† í”½ íŠ¹ì§•: {train_topics.shape[1]}")
    
    # íŠ¹ì§• í†µí•©
    print("\\nğŸ”— === íŠ¹ì§• í†µí•© ë° ìŠ¤ì¼€ì¼ë§ ===")
    
    # ê³ ê¸‰ íŠ¹ì§• ì •ê·œí™” (Robust Scalerë¡œ ì´ìƒì¹˜ì— ê°•í•˜ê²Œ)
    scaler = RobustScaler()
    train_advanced_scaled = scaler.fit_transform(train_advanced)
    test_advanced_scaled = scaler.transform(test_advanced)
    
    # í† í”½ íŠ¹ì§• ì •ê·œí™”
    topic_scaler = StandardScaler()
    train_topics_scaled = topic_scaler.fit_transform(train_topics)
    test_topics_scaled = topic_scaler.transform(test_topics)
    
    # ëª¨ë“  íŠ¹ì§•ì„ í•˜ë‚˜ë¡œ í†µí•©
    X_train_all = hstack([
        train_word_tfidf,      # ë‹¨ì–´ ë ˆë²¨ ì˜ë¯¸
        train_char_tfidf,      # ë¬¸ì ë ˆë²¨ ìŠ¤íƒ€ì¼
        train_advanced_scaled, # ê³ ê¸‰ ì–¸ì–´í•™ì  íŠ¹ì§•
        train_topics_scaled    # ì£¼ì œ ë¶„í¬
    ])
    
    X_test_all = hstack([
        test_word_tfidf,
        test_char_tfidf,
        test_advanced_scaled,
        test_topics_scaled
    ])
    
    y = train['generated']
    
    print(f"ğŸ¯ ìµœì¢… íŠ¹ì§• ìˆ˜: {X_train_all.shape[1]:,}")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train_all.shape}")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_all, y, stratify=y, test_size=0.2, random_state=42
    )
    
    print("âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ!")
    
    # ë‹¤ì–‘í•œ ëª¨ë¸ í›ˆë ¨
    print("\\nğŸš€ === 5ê°œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨ ===")
    
    models = {}
    val_predictions = {}
    
    # 1. XGBoost - íŠ¸ë¦¬ ê¸°ë°˜ ì•™ìƒë¸”ì˜ ì™•
    print("ğŸŒ³ XGBoost í›ˆë ¨ ì¤‘...")
    xgb = XGBClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='auc',
        tree_method='hist'
    )
    xgb.fit(X_train, y_train)
    models['xgb'] = xgb
    val_predictions['xgb'] = xgb.predict_proba(X_val)[:, 1]
    print(f"âœ… XGBoost AUC: {roc_auc_score(y_val, val_predictions['xgb']):.4f}")
    
    # 2. LightGBM - ë¹ ë¥´ê³  ì •í™•í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
    print("âš¡ LightGBM í›ˆë ¨ ì¤‘...")
    lgbm = lgb.LGBMClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        objective='binary',
        metric='auc',
        verbose=-1
    )
    lgbm.fit(X_train, y_train)
    models['lgbm'] = lgbm
    val_predictions['lgbm'] = lgbm.predict_proba(X_val)[:, 1]
    print(f"âœ… LightGBM AUC: {roc_auc_score(y_val, val_predictions['lgbm']):.4f}")
    
    # 3. CatBoost - ë²”ì£¼í˜• íŠ¹ì§•ì— íŠ¹í™”
    print("ğŸ± CatBoost í›ˆë ¨ ì¤‘...")
    catb = CatBoostClassifier(
        iterations=500,
        depth=8,
        learning_rate=0.05,
        random_seed=42,
        verbose=False,
        eval_metric='AUC'
    )
    catb.fit(X_train, y_train)
    models['catb'] = catb
    val_predictions['catb'] = catb.predict_proba(X_val)[:, 1]
    print(f"âœ… CatBoost AUC: {roc_auc_score(y_val, val_predictions['catb']):.4f}")
    
    # 4. Random Forest - ë‹¤ì–‘ì„±ê³¼ ì•ˆì •ì„±
    print("ğŸŒ² Random Forest í›ˆë ¨ ì¤‘...")
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    models['rf'] = rf
    val_predictions['rf'] = rf.predict_proba(X_val)[:, 1]
    print(f"âœ… Random Forest AUC: {roc_auc_score(y_val, val_predictions['rf']):.4f}")
    
    # 5. Logistic Regression - ì„ í˜• ëª¨ë¸ì˜ ê°•ì 
    print("ğŸ“ˆ Logistic Regression í›ˆë ¨ ì¤‘...")
    lr = LogisticRegression(
        C=0.1,
        random_state=42,
        max_iter=1000,
        solver='liblinear'
    )
    lr.fit(X_train, y_train)
    models['lr'] = lr
    val_predictions['lr'] = lr.predict_proba(X_val)[:, 1]
    print(f"âœ… Logistic Regression AUC: {roc_auc_score(y_val, val_predictions['lr']):.4f}")
    
    # ìµœì  ì•™ìƒë¸”
    print("\\nğŸ¯ === ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ===")
    
    # ê° ëª¨ë¸ì˜ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
    aucs = {}
    for name, pred in val_predictions.items():
        auc = roc_auc_score(y_val, pred)
        aucs[name] = auc
    
    # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (ì§€ìˆ˜ì  ìŠ¤ì¼€ì¼ë§)
    total_weight = sum(np.exp(auc * 10) for auc in aucs.values())
    weights = {name: np.exp(auc * 10) / total_weight for name, auc in aucs.items()}
    
    print("ğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥ ë° ê°€ì¤‘ì¹˜:")
    for name in weights:
        print(f"  {name}: AUC={aucs[name]:.4f}, Weight={weights[name]:.3f}")
    
    # ê°€ì¤‘ ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = np.zeros(len(y_val))
    for name, pred in val_predictions.items():
        ensemble_pred += weights[name] * pred
    
    ensemble_auc = roc_auc_score(y_val, ensemble_pred)
    print(f"\\nğŸš€ ê°€ì¤‘ ì•™ìƒë¸” AUC: {ensemble_auc:.4f}")
    
    # ë‹¨ìˆœ í‰ê· ê³¼ ë¹„êµ
    simple_ensemble = np.mean(list(val_predictions.values()), axis=0)
    simple_auc = roc_auc_score(y_val, simple_ensemble)
    print(f"ğŸ“Š ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” AUC: {simple_auc:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ì„ íƒ
    best_approach = 'weighted' if ensemble_auc > simple_auc else 'simple'
    best_auc = max(ensemble_auc, simple_auc)
    print(f"\\nğŸ† ì„ íƒëœ ì•™ìƒë¸” ë°©ì‹: {best_approach} (AUC: {best_auc:.4f})")
    
    # ìµœì¢… ì˜ˆì¸¡
    print("\\nğŸ”® === ìµœì¢… ì˜ˆì¸¡ ì¤‘ ===")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    test_predictions = {}
    for name, model in models.items():
        test_predictions[name] = model.predict_proba(X_test_all)[:, 1]
        print(f"âœ… {name} ì˜ˆì¸¡ ì™„ë£Œ")
    
    # ìµœì  ì•™ìƒë¸” ì ìš©
    if best_approach == 'weighted':
        final_probs = np.zeros(len(test_predictions['xgb']))
        for name, pred in test_predictions.items():
            final_probs += weights[name] * pred
        print("ğŸ¯ ê°€ì¤‘ ì•™ìƒë¸” ì ìš©")
    else:
        final_probs = np.mean(list(test_predictions.values()), axis=0)
        print("ğŸ“Š ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ì ìš©")
    
    print(f"\\nğŸ“ˆ ìµœì¢… ì˜ˆì¸¡ ë¶„í¬:")
    print(f"  ìµœì†Œê°’: {final_probs.min():.4f}")
    print(f"  ìµœëŒ€ê°’: {final_probs.max():.4f}")
    print(f"  í‰ê· ê°’: {final_probs.mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {final_probs.std():.4f}")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    sample_submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')
    sample_submission['generated'] = final_probs
    
    filename = './ultimate_ensemble_submission.csv'
    sample_submission.to_csv(filename, index=False)
    
    print(f"\\nğŸ‰ ìµœê³  ì„±ëŠ¥ ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {filename}")
    print(f"ğŸš€ ì˜ˆìƒ ì„±ëŠ¥: AUC {best_auc:.4f}")
    print(f"ğŸ’ª ì´ë²ˆì—” ë°˜ë“œì‹œ ì„±ê³µ!")

if __name__ == "__main__":
    main()
